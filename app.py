import streamlit as st
import sys
from dotenv import load_dotenv
import os
import re
import json
import ollama
import chromadb
import frontmatter
from neo4j import GraphDatabase

# ================= 0. åŠ è½½ç¯å¢ƒå˜é‡ =================
load_dotenv()  # è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶

# ================= 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½® =================
# ä½¿ç”¨ os.getenv('å˜é‡å', 'é»˜è®¤å€¼') çš„æ–¹å¼è¯»å–
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")  # å¯†ç ä¸å»ºè®®è®¾é»˜è®¤å€¼

CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chemical_kb")

EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text:latest")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-r1:latest")

# è·å–æœ¬åœ°æ•°æ®è·¯å¾„
DEFAULT_DATA_PATH = os.getenv("DATA_PATH", "./data/")

# ================= 2. åˆå§‹åŒ–è¿æ¥ =================
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
# --- åœ¨è¿™é‡Œæ’å…¥ã€é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°ã€‘ ---
def recursive_split_text(text, max_chars=1200, overlap=200):
    """å½“æ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰ä¼˜å…ˆçº§è¿›è¡Œé€’å½’æ‹†åˆ†"""
    if len(text) <= max_chars:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunk = text[start:end]
        if end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œæˆ–å¥å·ï¼Œé¿å…ç”Ÿç¡¬åˆ‡æ–­
            last_break = max(chunk.rfind('\n'), chunk.rfind('ã€‚'), chunk.rfind('. '))
            if last_break > max_chars * 0.5: 
                end = start + last_break + 1
                chunk = text[start:end]
        chunks.append(chunk)
        start += (len(chunk) - overlap)
        if len(chunk) <= overlap: break
    return chunks

def extract_tags(text):
    # 1. åŒ¹é…æ¨¡å¼ï¼š
    # ([a-zA-Z]{1,3})  -> æ•è·1-3ä½å­—æ¯å‰ç¼€ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # [-_]?            -> åŒ¹é…å¯é€‰çš„è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿
    # (\d{2,4})        -> æ•è·2-4ä½æ•°å­—
    # ([a-zA-Z]?)      -> æ•è·å¯é€‰çš„ä¸€ä½å­—æ¯åç¼€
    pattern = r'([a-zA-Z]{1,3})[\s\-_]?(\d{2,4})([a-zA-Z]?)'
    # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç»„åˆ
    matches = re.findall(pattern, text)
    
    normalized_tags = []
    for prefix, digits, suffix in matches:
        # 2. æ ‡å‡†åŒ–å¤„ç†ï¼š
        # å…¨éƒ¨è½¬ä¸ºå¤§å†™ï¼Œå¹¶åœ¨å­—æ¯ä¸æ•°å­—ä¹‹é—´å¼ºåˆ¶åŠ ä¸Š "-"
        # ä¾‹å­ï¼šd43 -> D-43, D_43 -> D-43, d-43a -> D-43A
        standard_tag = f"{prefix.upper()}-{digits}{suffix.upper()}"
        normalized_tags.append(standard_tag)
    
    # è¿”å›å»é‡åçš„ç»“æœ
    return list(set(normalized_tags))
def is_noise_node(node_labels, node_tag):
    """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦å¿½ç•¥çš„è¾…åŠ©èŠ‚ç‚¹ï¼ˆå¦‚ä¸‰é€šã€è·¨é¡µç¬¦ã€ä»ªè¡¨ï¼‰"""
    ignore_labels = ['OffPageConnector', 'Instrument', 'Drawing']
    ignore_tags = ['TEE', 'TappingPoint']
    # æ£€æŸ¥æ ‡ç­¾æˆ–ä½å·æ˜¯å¦åŒ…å«å¿½ç•¥å…³é”®è¯
    if any(l in node_labels for l in ignore_labels): return True
    if any(t in str(node_tag).upper() for t in ignore_tags): return True
    return False

def clean_markdown(content):
    content = re.sub(r'^\\---', '---', content, flags=re.MULTILINE)
    content = re.sub(r'^\\(#+)', r'\1', content, flags=re.MULTILINE)
    content = re.sub(r'[\u200B-\u200D\uFEFF]', '', content)
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.match(r'^\|[\s*:|-]+\|$', line.strip()): continue 
        if line.strip().startswith('|'):
            line = line.strip('|').replace('|', '   ')
        cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)

def hierarchical_chunking(content, file_path):
    file_name = os.path.basename(file_path).replace('.md', '')
    post = frontmatter.loads(content)
    doc_metadata = post.metadata
    main_content = post.content
    doc_title = doc_metadata.get('title', file_name)
    final_chunks = []
    
    # è®¾å®šå‚æ•°
    MAX_CHUNK_LEN = 1200 
    OVERLAP_LEN = 200

    h3_blocks = re.split(r'(?=^###\s+)', main_content, flags=re.MULTILINE)
    for i, h3_block in enumerate(h3_blocks):
        h3_block = h3_block.strip()
        if not h3_block: continue
        
        h3_match = re.search(r'^###\s+(.*)$', h3_block, flags=re.MULTILINE)
        h3_title = h3_match.group(1).strip() if h3_match else "æ¦‚è§ˆ"
        h3_content = re.sub(r'^###\s+.*$', '', h3_block, flags=re.MULTILINE).strip()
        
        if '#### ' in h3_content:
            h4_blocks = re.split(r'(?=^####\s+)', h3_content, flags=re.MULTILINE)
            for j, h4_block in enumerate(h4_blocks):
                h4_block = h4_block.strip()
                if len(h4_block) < 20: continue
                
                h4_match = re.search(r'^####\s+(.*)$', h4_block, flags=re.MULTILINE)
                h4_title = h4_match.group(1).strip() if h4_match else ""
                content_body = re.sub(r'^####\s+.*$', '', h4_block, flags=re.MULTILINE).strip()
                
                breadcrumb = f"{doc_title} > {h3_title}" + (f" > {h4_title}" if h4_title else "")
                
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(content_body, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-{j}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
        else:
            if len(h3_content) > 20:
                breadcrumb = f"{doc_title} > {h3_title}"
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(h3_content, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
    return final_chunks

def analyze_intent_with_llm(prompt, extracted_tags):
    # å°†æ¨¡å‹æç¤ºè¯ä¹Ÿæ”¹ä¸ºä¸­æ–‡ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å‡†ç¡®åœ°æŒ‰ä¸­æ–‡é€»è¾‘æ€è€ƒ
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šä¸“å®¶çº§æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾å¹¶è¿”å› JSONã€‚
    
    æ„å›¾åˆ†ç±»æ ‡å‡†ï¼š
    1. Path_Analysis: è¯¢é—®å·¥è‰ºæµç¨‹ã€ç‰©æ–™æµå‘ã€è·¯å¾„ã€ç»è¿‡å“ªäº›è®¾å¤‡ã€è·¨é¡µæµç¨‹ã€‚
    2. Fault_Diagnosis: è¯¢é—®æ•…éšœåŸå› ã€ä¸Šæ¸¸æº¯æºã€ä¸²æ–™åˆ†æã€å¼‚å¸¸æ³¢åŠ¨æ¥æºã€‚
    3. Status_Check: è¯¢é—®è®¾å¤‡å·¥ä½œåŸç†ã€è®¾è®¡å‚æ•°ï¼ˆå‹åŠ›/æ¸©åº¦/æè´¨ï¼‰ã€ç›‘æ§ä»ªè¡¨ä½å·ã€é‡ç¨‹ã€‚
    4. Procedure_Query: è¯¢é—®æ“ä½œæ­¥éª¤ã€å¯åŠ¨/åœæ­¢é¡ºåºã€å®‰å…¨æ³¨æ„äº‹é¡¹ã€SOPã€‚
    5. Info_Query: è¯¢é—®åŸºæœ¬å®šä¹‰ã€æœ¯è¯­è§£é‡Šã€é€šç”¨å¸¸è¯†ã€‚

    ç”¨æˆ·æå–ä½å·ï¼š{extracted_tags}
    è¿”å›æ ¼å¼ï¼š{{"intent": "æ„å›¾åç§°", "start_node": "èµ·ç‚¹ä½å·", "end_node": "ç»ˆç‚¹ä½å·", "target_tag": "ç›®æ ‡ä½å·"}}"""
    
    try:
        response = ollama.chat(model=LLM_MODEL, messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': prompt}
        ], format='json')
        return json.loads(response['message']['content'])
    except:
        return {"intent": "Info_Query"}

def build_cypher(llm_result, extracted_tags, user_text):
    intent = llm_result.get("intent", "Info_Query")
    tags = extracted_tags
    
    # è·¯å¾„åˆ†æå¼ºåˆ¶ä¿®æ­£
    if len(tags) >= 2 and any(k in user_text for k in ["åˆ°", "æµ", "ç»è¿‡", "å»å¾€", "æµç¨‹"]):
        intent = "Path_Analysis"
        
    cypher = ""; params = {}

    if intent == "Path_Analysis":
        start = llm_result.get("start_node") or (tags[0] if tags else None)
        end = llm_result.get("end_node") or (tags[1] if len(tags)>1 else None)
        
        if start and end:
            cypher = """
            MATCH (start:Asset), (end:Asset)
            WHERE (start.Tag STARTS WITH $startTag OR replace(start.Tag, '-', '') = $startTagAlt)
              AND (end.Tag STARTS WITH $endTag OR replace(end.Tag, '-', '') = $endTagAlt)
            
            // 1. æ‰¾åˆ°ä¸»å¹²è·¯å¾„
            MATCH path = shortestPath((start)-[:PIPE|LINKS_TO*..60]->(end))
            
            // 2. æ‹†è§£è·¯å¾„ï¼Œå¯¹æ¯ä¸ªèŠ‚ç‚¹å’Œå…³ç³»è¿›è¡Œè¯­ä¹‰æå–
            WITH nodes(path) AS ns, relationships(path) AS rs
            UNWIND range(0, size(ns)-1) AS i
            WITH i, ns[i] AS n, rs, size(ns) AS pathLen
            
            // 3. æŠ“å–ä»ªè¡¨å’Œæ§åˆ¶å›è·¯ (åŒå‘ MEASURES)
            OPTIONAL MATCH (n)-[:MEASURES]-(inst:Instrument)
            OPTIONAL MATCH (inst)-[:CONTROLS]->(v:Valve)
            
            // 4. æŒ‰æ­¥éª¤èšåˆè¯­ä¹‰ä¿¡æ¯
            WITH i, n, rs, 
                 collect(DISTINCT {
                     tag: inst.Tag, 
                     desc: inst.desc, 
                     type: inst.type,
                     controls: v.Tag
                 }) AS step_instruments
            ORDER BY i
            
            // 5. è¿”å›ç»“æ„åŒ–æ•°æ®
            RETURN 'Path_Analysis' as intent,
                   collect({
                       step: i,
                       node: {
                           tag: n.Tag, 
                           desc: n.desc, 
                           labels: labels(n)
                       },
                       instruments: [inst in step_instruments WHERE inst.tag IS NOT NULL],
                       next_rel: CASE WHEN i < size(rs) THEN properties(rs[i]) ELSE null END
                   }) AS path_steps,
                   size(rs) as total_hops
            """
            params = {
                "startTag": start, 
                "startTagAlt": start.replace("-", ""), 
                "endTag": end, 
                "endTagAlt": end.replace("-", "")
            }

    elif intent in ["Status_Check", "Info_Query", "Fault_Diagnosis"]:
        cypher = """
        UNWIND $tags AS qTag
        MATCH (e:Asset) WHERE e.Tag = qTag OR replace(e.Tag, '-', '') = replace(qTag, '-', '')
        
        // 1. æœç´¢ 1-5 æ­¥ä»¥å†…çš„é‚»åŸŸè·¯å¾„
        OPTIONAL MATCH p = (e)-[:PIPE|MEASURES|CONTROLS|LINKS_TO*1..5]-(m)
        
        // 2. æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ï¼š
        // - è·¯å¾„ä¸­é—´èŠ‚ç‚¹(nodes(p)[1..-2])ä¸èƒ½æ˜¯ä¸»å·¥è‰ºè®¾å¤‡ï¼ˆç©¿è¿‡é™„ä»¶ï¼Œæ­¢äºè®¾å¤‡ï¼‰
        // - ç»ˆç‚¹èŠ‚ç‚¹ m ä¸èƒ½æ˜¯ä¸‰é€š (TEE)
        WHERE (size(nodes(p)) = 1 OR 
              ALL(n IN nodes(p)[1..size(nodes(p))-2] 
                  WHERE NOT n:Reactor AND NOT n:Pump AND NOT n:Tank AND NOT n:Exchanger AND NOT n:Tower)
              )
          AND NOT m:TEE AND m.Tag <> 'TEE'
        
        // 3. æå–å…³ç³»å±æ€§ï¼ˆç”¨äºåˆ¤æ–­æµå‘å’Œä»‹è´¨ï¼‰
        WITH e, m, p, last(relationships(p)) AS lastRel
        
        RETURN e.Tag as center_tag,
               e.desc as center_desc,
               properties(e) as center_params,
               collect(DISTINCT {
                   node_tag: m.Tag,
                   node_desc: m.desc,
                   node_labels: labels(m),
                   rel_type: type(lastRel),
                   rel_props: properties(lastRel),
                   distance: length(p)
               }) AS neighborhood
        """
        params = {"tags": tags}

    return cypher, params

def query_neo4j(query, params):
    if not query: return []
    # ç»ˆç«¯è°ƒè¯•ä¿¡æ¯ä¿ç•™è‹±æ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    print(f"\n[è°ƒè¯•] æ‰§è¡Œ Cypher: {query}\n[è°ƒè¯•] å‚æ•°: {params}", file=sys.stderr, flush=True)
    try:
        with neo4j_driver.session() as session:
            result = session.run(query, **params)
            return [dict(record) for record in result]
    except Exception as e:
        print(f"[é”™è¯¯] Neo4j æŸ¥è¯¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
        return []
# ==============================================================================
# ğŸ‘‡ğŸ‘‡ğŸ‘‡ è¯·åœ¨è¿™é‡Œæ’å…¥æ–°å¢çš„è¾…åŠ©å‡½æ•° ğŸ‘‡ğŸ‘‡ğŸ‘‡
# ==============================================================================

def translate_region(region_code):
    """å°†è‹±æ–‡åŒºåŸŸä»£ç ç¿»è¯‘ä¸ºä¸­æ–‡è¯­ä¹‰"""
    if not region_code: return "é€šç”¨æ¥å£"
    mapping = {
        'ShellSide': 'å£³ç¨‹',
        'ShellSide:Vapor': 'å£³ç¨‹(æ°”ç›¸)',
        'ShellSide:Liquid': 'å£³ç¨‹(æ¶²ç›¸)',
        'TubeSide': 'ç®¡ç¨‹',
        'TubeSide:Liquid': 'ç®¡ç¨‹(æ¶²ç›¸)',
        'TubeSide:Vapor': 'ç®¡ç¨‹(æ°”ç›¸)',
        'Jacket': 'å¤¹å¥—',
        'InnerVessel': 'å†…èƒ†',
        'ControlSignal': 'æ§åˆ¶ä¿¡å·æ¥å£',
        'UpperSaltChannel': 'ä¸Šç›é“',
        'LowerSaltChannel': 'ä¸‹ç›é“'
    }
    return mapping.get(region_code, region_code)

def format_graph_data(data, intent):
    """
    é’ˆå¯¹æœ¬åœ°å¤§æ¨¡å‹ä¼˜åŒ–çš„ç»“æ„åŒ–å™è¿°ç”Ÿæˆå™¨
    åŠŸèƒ½ï¼šå»é‡ã€é€»è¾‘åˆ†ç»„ã€è¯­ä¹‰å¢å¼ºã€æ¶ˆé™¤åæ ‡å™ªéŸ³
    """
    if not data:
        return "æœªåœ¨å›¾è°±ä¸­æ‰¾åˆ°ç›¸å…³ä½å·çš„æ‹“æ‰‘è®°å½•ã€‚"

    text_lines = []

    for record in data:
        c_tag = record.get('center_tag')
        c_desc = record.get('center_desc') or "æœªå‘½åè®¾å¤‡"
        c_params = record.get('center_params', {})
        neighborhood = record.get('neighborhood', [])

        # --- 1. è®¾å¤‡æ ¸å¿ƒæ¡£æ¡ˆ ---
        text_lines.append(f"### ğŸ—ï¸ è®¾å¤‡æ ¸å¿ƒæ¡£æ¡ˆ: {c_tag} ({c_desc})")
        
        # è¿‡æ»¤æ‰æ— ç”¨çš„åæ ‡å’ŒUIå±æ€§
        ignore_keys = ['layout', 'x6Id', 'labelPosition', 'drawingId', 'Tag', 'desc', 'id', 'type']
        params_list = [f"{k}: **{v}**" for k, v in c_params.items() if k not in ignore_keys and v]
        if params_list:
            text_lines.append(f"- **åŸºæœ¬å‚æ•°**: {' | '.join(params_list)}")
        
        # --- 2. æ•°æ®é¢„å¤„ç†ï¼ˆå»é‡ä¸åˆ†ç»„ï¼‰ ---
        pipes_by_fluid = {}  # æŒ‰ä»‹è´¨åˆ†ç»„ç®¡é“
        instruments = {}     # ä»ªè¡¨å»é‡
        control_loops = []   # æ§åˆ¶å›è·¯
        links = set()        # è·¨é¡µè¿æ¥å»é‡

        for item in neighborhood:
            tag = item.get('node_tag')
            if not tag and not item.get('node_desc'): continue # è·³è¿‡æ— åèŠ‚ç‚¹
            
            rel_type = item.get('rel_type')
            props = item.get('rel_props', {})
            
            # A. å¤„ç†ç®¡é“ (æŒ‰ä»‹è´¨åˆ†ç»„)
            if rel_type == 'PIPE':
                fluid = props.get('fluid', 'å…¶ä»–ä»‹è´¨')
                if fluid not in pipes_by_fluid: pipes_by_fluid[fluid] = {}
                
                # åœ¨ä»‹è´¨ç»„å†…æŒ‰ä½å·å»é‡
                if tag not in pipes_by_fluid[fluid]:
                    pipes_by_fluid[fluid][tag] = {
                        'desc': item.get('node_desc'),
                        'dn': props.get('dn'),
                        'path': f"{translate_region(props.get('fromRegion'))} â” {translate_region(props.get('toRegion'))}"
                    }

            # B. å¤„ç†ä»ªè¡¨ (æŒ‰ä½å·å»é‡)
            elif rel_type == 'MEASURES':
                if tag not in instruments:
                    instruments[tag] = {
                        'desc': item.get('node_desc'),
                        'points': set()
                    }
                if props.get('fromPort'):
                    instruments[tag]['points'].add(props.get('fromPort'))

            # C. å¤„ç†æ§åˆ¶å›è·¯
            elif rel_type == 'CONTROLS':
                control_loops.append(f"æ§åˆ¶ä¿¡å·: **{tag}** ({item.get('node_desc')}) â” ä½œç”¨äºæ‰§è¡Œæœºæ„")

            # D. å¤„ç†è·¨é¡µè¿æ¥
            elif rel_type == 'LINKS_TO' or 'OffPageConnector' in item.get('node_labels', []):
                links.add(f"**{tag}** ({item.get('node_desc') or 'è·¨é¡µè¿½è¸ªç‚¹'})")

        # --- 3. æ„é€ ç»“æ„åŒ–å™è¿° ---

        # 3.1 å·¥è‰ºç‰©æ–™è·¯å¾„
        if pipes_by_fluid:
            text_lines.append("\n#### ğŸŒŠ å·¥è‰ºç‰©æ–™è·¯å¾„ (Process Connections)")
            for fluid, nodes in pipes_by_fluid.items():
                text_lines.append(f"- **ä»‹è´¨: {fluid}**")
                for n_tag, n_info in nodes.items():
                    dn_str = f" [{n_info['dn']}]" if n_info['dn'] else ""
                    text_lines.append(f"  - å…³è”è®¾å¤‡: **{n_tag}** ({n_info['desc']}){dn_str} | é€»è¾‘: {n_info['path']}")

        # 3.2 ç›‘æµ‹ä¸æ§åˆ¶é€»è¾‘
        if instruments or control_loops:
            text_lines.append("\n#### ğŸ›¡ï¸ ç›‘æµ‹ä¸æ§åˆ¶é€»è¾‘ (Instrumentation)")
            # ä»ªè¡¨
            for i_tag, i_info in instruments.items():
                pts = f" [æµ‹ç‚¹: {', '.join(i_info['points'])}]" if i_info['points'] else ""
                text_lines.append(f"- ğŸ“¥ [æµ‹é‡] **{i_tag}** ({i_info['desc']}){pts}")
            # å›è·¯
            for loop in list(set(control_loops)): # å»é‡æ˜¾ç¤º
                text_lines.append(f"- ğŸ“¤ [æ§åˆ¶] {loop}")

        # 3.3 è·¨é¡µå»¶ç»­
        if links:
            text_lines.append("\n#### ğŸ“‘ è·¨å›¾çº¸å»¶ç»­")
            text_lines.append(f"- ç»­æ¥ç‚¹: {', '.join(links)}")

        text_lines.append("\n---\n")

    return "\n".join(text_lines)
# ==============================================================================
# ğŸ‘†ğŸ‘†ğŸ‘† æ’å…¥ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
# ============================================================================

# ================= 4. Streamlit ç•Œé¢æ˜¾ç¤º (ä¸­æ–‡ä¸­æ–‡åŒ–) =================
st.set_page_config(page_title="åŒ–å·¥çŸ¥è¯†å›¾è°±", layout="wide", page_icon="ğŸ§ª")
st.title("ğŸ§ª åŒ–å·¥è£…ç½®å›¾è°± + æ–‡æ¡£å‘é‡æ··åˆçŸ¥è¯†åº“")

# --- ä¾§è¾¹æ ï¼šç®¡ç†é¢æ¿ ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç³»ç»Ÿåå°ç®¡ç†")
    
    # æ˜¾ç¤ºå‘é‡åº“ç»Ÿè®¡
    try:
        db_count = collection.count()
        st.metric("å·²å­˜å‚¨çŸ¥è¯†åˆ‡ç‰‡æ•°", db_count)
    except:
        st.error("å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    st.markdown("---")
    
    # çŸ¥è¯†åŒæ­¥åŠŸèƒ½
    st.subheader("ğŸ“ æ•°æ®åŒæ­¥")
    path_input = st.text_area("Markdown æ–‡æ¡£è·¯å¾„", "/Users/chenfeng/åŸ¹è®­èµ„æ–™/åŒ–å·¥è‹¯é…åŸºæœ¬çŸ¥è¯†/")
    
    if st.button("ğŸš€ å¼€å§‹å¢é‡åŒæ­¥"):
        all_chunks = []
        with st.spinner("æ­£åœ¨æ‰«æå¹¶è§£ææ–‡æ¡£..."):
            if os.path.exists(path_input):
                for root, _, files in os.walk(path_input):
                    for file in files:
                        if file.endswith('.md'):
                            try:
                                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                                    cleaned = clean_markdown(f.read())
                                    chunks = hierarchical_chunking(cleaned, os.path.join(root, file))
                                    all_chunks.extend(chunks)
                            except: pass
            
        if all_chunks:
            total = len(all_chunks)
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, chunk in enumerate(all_chunks):
                status_text.text(f"æ­£åœ¨å‘é‡åŒ– ({idx+1}/{total}): {chunk['id']}")
                try:
                    emb = ollama.embeddings(model=EMBED_MODEL, prompt=chunk['text'][:2000])['embedding']
                    collection.upsert(
                        ids=[chunk['id']], 
                        embeddings=[emb], 
                        documents=[chunk['text']], 
                        metadatas=[{k:str(v) for k,v in chunk['metadata'].items()}]
                    )
                except: pass
                progress_bar.progress((idx + 1) / total)
            
            status_text.text("âœ… åŒæ­¥åœ†æ»¡å®Œæˆï¼")
            st.balloons()
            st.rerun()
        else:
            st.warning("æœªåœ¨è¯¥è·¯å¾„ä¸‹æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")

    if st.button("ğŸ—‘ï¸ å±é™©æ“ä½œï¼šæ¸…ç©ºå‘é‡åº“"):
        chroma_client.delete_collection(COLLECTION_NAME)
        collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
        st.warning("åº“å·²æ¸…ç©º")
        st.rerun()

# --- å¯¹è¯åŒºåŸŸ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥
# --- å¯¹è¯åŒºåŸŸ (ä¼˜åŒ–ç‰ˆ) ---
if prompt := st.chat_input("æ‚¨å¯ä»¥é—®æˆ‘ï¼šD-14 ååº”å™¨çš„è®¾è®¡å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 1. æ£€ç´¢é˜¶æ®µ (ChromaDB + Neo4j)
        graph_data = []
        vector_docs = []
        
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢åŒåº“äº‹å®...", expanded=True) as status:
            extracted_tags = extract_tags(prompt)
            # ã€ä¿®æ­£ç‚¹ 1ã€‘ï¼šå…ˆè¯†åˆ«æ„å›¾
            intent_res = analyze_intent_with_llm(prompt, extracted_tags)
            current_intent = intent_res.get('intent', 'Info_Query') # è·å–çœŸå®æ„å›¾
            st.write(f"ğŸ·ï¸ **è¯†åˆ«ä½å·**: `{', '.join(extracted_tags)}` | ğŸ¯ **æ„å›¾**: `{current_intent}`")
            
            # ã€ä¿®æ­£ç‚¹ 2ã€‘ï¼šæ ¹æ®çœŸå®æ„å›¾å†³å®šæ£€ç´¢æ–‡æ¡£æ•°
            n_docs = 6 if current_intent == "Procedure_Query" else 3
            
            cypher, params = build_cypher(intent_res, extracted_tags, prompt)
            if cypher:
                graph_data = query_neo4j(cypher, params)
            
            q_emb = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)['embedding']
            vector_res = collection.query(query_embeddings=[q_emb], n_results=n_docs)
            vector_docs = vector_res['documents'][0]
            
            status.update(label=f"âœ… æ£€ç´¢å®Œæˆ: {current_intent}", state="complete", expanded=False)

        # 2. å›ç­”ç”Ÿæˆé˜¶æ®µ
        full_response = ""
        
        # --- ã€åŠ¨æ€å›¾æ ‡æ–¹æ¡ˆã€‘åˆ›å»ºä¸€ä¸ªåŒ–å­¦/AI åŠ¨æ€å ä½ç¬¦ ---
        thinking_container = st.empty()
        
        with thinking_container.container():
            st.markdown(
                """
                <style>
                @keyframes pulse-ring {
                  0% { transform: scale(.33); }
                  80%, 100% { opacity: 0; }
                }
                @keyframes pulse-dot {
                  0% { transform: scale(.8); }
                  50% { transform: scale(1); }
                  100% { transform: scale(.8); }
                }
                .ai-thinking-container {
                  display: flex;
                  flex-direction: column;
                  align-items: center;
                  justify-content: center;
                  padding: 20px;
                }
                .pulse-wrapper {
                  position: relative;
                  width: 60px;
                  height: 60px;
                }
                .pulse-dot {
                  position: absolute;
                  top: 15px; left: 15px;
                  width: 30px; height: 30px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-dot 1.25s cubic-bezier(0.455, 0.03, 0.515, 0.955) -.4s infinite;
                  display: flex;
                  align-items: center;
                  justify-content: center;
                  z-index: 2;
                }
                .pulse-ring {
                  position: absolute;
                  top: 0; left: 0;
                  width: 60px; height: 60px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-ring 1.25s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
                  z-index: 1;
                }
                .thinking-text {
                  margin-top: 15px;
                  font-family: sans-serif;
                  color: #007bff;
                  font-weight: bold;
                  letter-spacing: 1px;
                }
                </style>
                
                <div class="ai-thinking-container">
                    <div class="pulse-wrapper">
                        <div class="pulse-ring"></div>
                        <div class="pulse-dot">
                            <span style="font-size: 18px;">ğŸ§ª</span>
                        </div>
                    </div>
                    <div class="thinking-text">åŠªåŠ›åˆ†æä¸­...</div>
                </div>
                """, 
                unsafe_allow_html=True
            )


        # æœ€ç»ˆå›ç­”æ‰“å­—æœºæ˜¾ç¤ºçš„å ä½ç¬¦
        response_placeholder = st.empty()
        
        if not graph_data and not vector_docs:
            thinking_container.empty() # æ²¡æ‰¾åˆ°æ•°æ®ï¼Œç›´æ¥æ¸…é™¤æç¤º
            response_placeholder.warning("âš ï¸ æ ¹æ®ç›®å‰çŸ¥è¯†åº“è®°å½•ï¼Œæœªæ‰¾åˆ°ä¸è¯¥æé—®ç›¸å…³çš„ä½å·äº‹å®æˆ–æ–‡æ¡£è¯´æ˜ã€‚")
        else:
             # === [ä¿®æ”¹å¼€å§‹] ä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•° ===
            
            # 1. å°†å›¾æ•°æ®è½¬æ¢ä¸ºé“¾å¼å™è¿°æ–‡æœ¬
            current_intent = intent_res.get('intent', 'Info_Query')
            graph_text_narrative = format_graph_data(graph_data, current_intent)
            
            # 2. æ„é€ æ›´æ¸…æ™°çš„ä¸Šä¸‹æ–‡
            h_context = f"""
ã€å›¾è°±äº‹å® (ç‰©ç†æ‹“æ‰‘ä¸å·¥è‰ºè¯­ä¹‰)ã€‘:
{graph_text_narrative}

ã€çŸ¥è¯†åº“æ–‡æ¡£ (æ“ä½œè§„ç¨‹ä¸åŸç†)ã€‘:
{' '.join(vector_docs)}
            """
            # === [ä¿®æ”¹ç»“æŸ] ===
            
            # --- æç¤ºè¯å¾®è°ƒ (ç¡®ä¿æ¨¡å‹ä¸ä¼šå¤ªå•°å—¦) ---
            intent_guidance = {
            "Path_Analysis": "å½“å‰ä»»åŠ¡æ˜¯ã€å·¥è‰ºæµç¨‹åˆ†æã€‘ï¼Œè¯·é‡ç‚¹æè¿°è®¾å¤‡çš„ä½œç”¨ã€ç‰©æ–™æµå‘ã€è…”å®¤åˆ‡æ¢åŠä»‹è´¨å˜åŒ–ã€‚",
            "Fault_Diagnosis": "å½“å‰ä»»åŠ¡æ˜¯ã€æ•…éšœè¯Šæ–­ã€‘ï¼Œè¯·åˆ†æä¸Šæ¸¸å¯èƒ½çš„é£é™©æºã€‚",
            "Status_Check": "å½“å‰ä»»åŠ¡æ˜¯ã€çŠ¶æ€æ£€æŸ¥ã€‘ï¼Œè¯·æ ¸å¯¹è®¾å¤‡å‚æ•°ä¸ä»ªè¡¨ç›‘æ§èŒƒå›´ã€‚",
            "Procedure_Query": "å½“å‰ä»»åŠ¡æ˜¯ã€è§„ç¨‹æŸ¥è¯¢ã€‘ï¼Œè¯·è¯¦ç»†è¯´æ˜æ“ä½œæ­¥éª¤å’Œå®‰å…¨è¦æ±‚ã€‚",
            "Info_Query": "å½“å‰ä»»åŠ¡æ˜¯ã€ä¿¡æ¯æŸ¥è¯¢ã€‘ï¼Œè¯·è§£é‡Šç›¸å…³ä½å·çš„åŠŸèƒ½å®šä¹‰ã€‚"
        }
        task_context = intent_guidance.get(current_intent, "")

            # ä¿®æ”¹åŸæœ‰çš„ sys_promptï¼Œåœ¨å¼€å¤´æ³¨å…¥ task_context
        sys_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„åŒ–å·¥è£…ç½®å·¥è‰ºå·¥ç¨‹å¸ˆã€‚
                
                ### 0. å½“å‰ä»»åŠ¡é‡ç‚¹
                {task_context}


                ### 1. çŸ¥è¯†å›´æ  (Knowledge Guardrails) - æ ¸å¿ƒå‡†åˆ™
                - **ä»…é™ä¸Šä¸‹æ–‡å›ç­”**ï¼šä½ åªèƒ½æ ¹æ®ä¸‹æ–¹æä¾›çš„ã€å›¾è°±äº‹å®ã€‘å’Œã€æ–‡æ¡£èµ„æ–™ã€‘è¿›è¡Œå›ç­”ã€‚ä¸¥ç¦ä½¿ç”¨ä½ è‡ªèº«è®­ç»ƒæ•°æ®ä¸­å…³äºç‰¹å®šå·¥å‚ã€ç‰¹å®šä½å·çš„å¤–éƒ¨çŸ¥è¯†ã€‚
                - **ä¸¥ç¦æ¨æµ‹è¿æ¥**ï¼šå¦‚æœã€å›¾è°±äº‹å®ã€‘ä¸­æ²¡æœ‰æ˜¾ç¤º A è®¾å¤‡ä¸ B è®¾å¤‡ä¹‹é—´çš„è·¯å¾„ï¼Œå³ä½¿åœ¨å¸¸è§„å·¥è‰ºä¸­å®ƒä»¬é€šå¸¸ç›¸è¿ï¼Œä½ ä¹Ÿå¿…é¡»å›ç­”â€œå½“å‰å›¾è°±æœªè®°å½• A ä¸ B çš„ç›´æ¥è¿æ¥â€ã€‚
                - **è¯šå®å‘ŠçŸ¥ç¼ºå¤±**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®çš„ä½å·åœ¨ã€å›¾è°±äº‹å®ã€‘ä¸­ä¸å­˜åœ¨ï¼Œæˆ–è€…è¯¢é—®çš„æ“ä½œåœ¨ã€æ–‡æ¡£èµ„æ–™ã€‘ä¸­æœªæåŠï¼Œè¯·æ˜ç¡®å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰çŸ¥è¯†åº“è®°å½•ï¼Œæ— æ³•æä¾›å…³äº [ä½å·/æ“ä½œ] çš„ä¿¡æ¯â€ã€‚
                - **ç¦æ­¢å¹»è§‰è¡¥å…¨**ï¼šä¸¥ç¦ä¸ºäº†ä½¿æµç¨‹å®Œæ•´è€Œè‡ªè¡Œè¡¥å…¨ä¸­é—´çš„é˜€é—¨ã€ç®¡æ®µæˆ–ä»ªè¡¨ã€‚

                ### 2. è¯æ®æº¯æºè¦æ±‚
                - ä½ çš„æ¯ä¸€å¥å…³é”®ç»“è®ºéƒ½åº”æš—ç¤ºå…¶æ¥æºã€‚
                - æ¶‰åŠç‰©ç†è¿æ¥ã€ä»‹è´¨ã€æè´¨ã€è…”å®¤é€»è¾‘æ—¶ï¼Œè¯·è¡¨è¿°ä¸ºï¼šâ€œæ ¹æ®å›¾è°±æ‹“æ‰‘è®°å½•...â€ã€‚
                - æ¶‰åŠæ“ä½œæ­¥éª¤ã€å®‰å…¨è¦æ±‚ã€å·¥è‰ºåŸç†æ—¶ï¼Œè¯·è¡¨è¿°ä¸ºï¼šâ€œæ ¹æ®æ“ä½œè§„ç¨‹è®°è½½...â€ã€‚

                ### 3. ç‰©ç†è¯­ä¹‰çº¦æŸ
                - å¿…é¡»å°Šé‡è…”å®¤é€»è¾‘ï¼šæ˜ç¡®åŒºåˆ†å£³ç¨‹(ShellSide)ã€ç®¡ç¨‹(TubeSide)ã€å¤¹å¥—(Jacket)ã€‚å¦‚æœç‰©æ–™æµå‘äº†é”™è¯¯çš„è…”å®¤ï¼Œè¯·åœ¨å›ç­”ä¸­ä½œä¸ºæ½œåœ¨é£é™©ç‚¹æŒ‡å‡ºã€‚

                ### 4. å›ç­”é£æ ¼
                - é£æ ¼ï¼šæå…¶ä¸“ä¸šã€å†·å³»ã€å®¢è§‚ã€‚
                - ç»“æ„ï¼š
                1. ã€æ ¸å¿ƒç»“è®ºã€‘ï¼šä¸€å¥è¯ç›´æ¥å›ç­”é—®é¢˜ï¼ŒåŒ…æ‹¬è®¾å¤‡çš„ä½å·å’Œåç§°ã€‚
                2.  æ ¹æ®ä»»åŠ¡é‡ç‚¹è¯¦ç»†è®²è§£ã€‚
                3. ã€å®‰å…¨æé†’ã€‘ï¼šï¼ˆå¦‚æœ‰ï¼‰åŸºäºäº‹å®çš„é£é™©å‘ŠçŸ¥ã€‚

                ### 5. è´Ÿé¢çº¦æŸ (Negative Constraints)
                - ç»å¯¹ç¦æ­¢ä½¿ç”¨ï¼š â€œæˆ‘çŒœâ€ã€â€œé€šå¸¸æƒ…å†µä¸‹â€ã€â€œç»éªŒè¡¨æ˜â€ã€â€œå¯èƒ½â€ã€‚
                - ç»å¯¹ç¦æ­¢å›ç­”ï¼š ä¸å½“å‰è£…ç½®æ— å…³çš„é€šç”¨åŒ–å·¥å¸¸è¯†ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¯¢é—®å®šä¹‰ï¼‰ã€‚
                """

        try:
                # è°ƒç”¨æ¨¡å‹
                stream = ollama.chat(model=LLM_MODEL, messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': f"äº‹å®èƒŒæ™¯: {h_context}\né—®é¢˜: {prompt}"}
                ], stream=True)

                # --- æ ¸å¿ƒæ”¹è¿›ï¼šç²¾ç¡®æ§åˆ¶æ¸…é™¤æ—¶æœº ---
                for chunk in stream:
                    content = chunk['message']['content']
                    
                    # åªæœ‰å½“æ¨¡å‹çœŸæ­£è¾“å‡ºäº†å†…å®¹ï¼ˆä¸”éç©ºï¼‰æ—¶ï¼Œæ‰æ¸…é™¤â€œæ€è€ƒä¸­â€æç¤º
                    if content.strip() and not full_response:
                        thinking_container.empty()
                    
                    full_response += content
                    # åŠ¨æ€å±•ç¤ºæ‰“å­—æœºæ•ˆæœ
                    response_placeholder.markdown(full_response + "â–Œ")
                
                # å®Œæˆè¾“å‡ºï¼Œç§»é™¤å…‰æ ‡
                response_placeholder.markdown(full_response)
                
        except Exception as e:
                thinking_container.empty()
                st.error(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")

        # 3. è¯æ®æº¯æºæ˜¾ç¤º (ä¼˜åŒ–ç‰ˆ)
        if graph_data or vector_docs:
            with st.expander("ğŸ” åŸå§‹æ£€ç´¢è¯æ®"):
                tab1, tab2 = st.tabs(["å›¾è°±äº‹å® (é“¾å¼å™è¿°)", "æ–‡æ¡£ç‰‡æ®µ"])
                
                with tab1:
                    # === [ä¿®æ”¹] ä½¿ç”¨ format_graph_data æ¸²æŸ“ ===
                    if graph_data:
                        # å¤ç”¨ä¹‹å‰è®¡ç®—å¥½çš„ intent
                        current_intent = intent_res.get('intent', 'Info_Query')
                        formatted_text = format_graph_data(graph_data, current_intent)
                        st.markdown(formatted_text)
                    else:
                        st.info("æ— å›¾è°±æ•°æ®")
                
                with tab2:
                    if vector_docs:
                        for i, d in enumerate(vector_docs):
                            st.info(f"**ç‰‡æ®µ {i+1}**:\n{d}")
                    else:
                        st.info("æ— æ–‡æ¡£æ•°æ®")

    # å°†åŠ©æ‰‹çš„å›ç­”å­˜å…¥å¯¹è¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})