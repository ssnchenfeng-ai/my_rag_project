import streamlit as st
import sys
from dotenv import load_dotenv
import os
import re
import json
import ollama
import chromadb
import frontmatter
from neo4j import GraphDatabase

# ================= 0. åŠ è½½ç¯å¢ƒå˜é‡ =================
load_dotenv()  # è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶

# ================= 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½® =================
# ä½¿ç”¨ os.getenv('å˜é‡å', 'é»˜è®¤å€¼') çš„æ–¹å¼è¯»å–
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")  # å¯†ç ä¸å»ºè®®è®¾é»˜è®¤å€¼

CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chemical_kb")

EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text:latest")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-r1:latest")

# è·å–æœ¬åœ°æ•°æ®è·¯å¾„
DEFAULT_DATA_PATH = os.getenv("DATA_PATH", "./data/")

# ================= 2. åˆå§‹åŒ–è¿æ¥ =================
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
# --- åœ¨è¿™é‡Œæ’å…¥ã€é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°ã€‘ ---
def recursive_split_text(text, max_chars=1200, overlap=200):
    """å½“æ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰ä¼˜å…ˆçº§è¿›è¡Œé€’å½’æ‹†åˆ†"""
    if len(text) <= max_chars:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunk = text[start:end]
        if end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œæˆ–å¥å·ï¼Œé¿å…ç”Ÿç¡¬åˆ‡æ–­
            last_break = max(chunk.rfind('\n'), chunk.rfind('ã€‚'), chunk.rfind('. '))
            if last_break > max_chars * 0.5: 
                end = start + last_break + 1
                chunk = text[start:end]
        chunks.append(chunk)
        start += (len(chunk) - overlap)
        if len(chunk) <= overlap: break
    return chunks

def extract_tags(text):
    # 1. åŒ¹é…æ¨¡å¼ï¼š
    # ([a-zA-Z]{1,3})  -> æ•è·1-3ä½å­—æ¯å‰ç¼€ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # [-_]?            -> åŒ¹é…å¯é€‰çš„è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿
    # (\d{2,4})        -> æ•è·2-4ä½æ•°å­—
    # ([a-zA-Z]?)      -> æ•è·å¯é€‰çš„ä¸€ä½å­—æ¯åç¼€
    pattern = r'([a-zA-Z]{1,3})[\s\-_]?(\d{2,4})([a-zA-Z]?)'
    # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç»„åˆ
    matches = re.findall(pattern, text)
    
    normalized_tags = []
    for prefix, digits, suffix in matches:
        # 2. æ ‡å‡†åŒ–å¤„ç†ï¼š
        # å…¨éƒ¨è½¬ä¸ºå¤§å†™ï¼Œå¹¶åœ¨å­—æ¯ä¸æ•°å­—ä¹‹é—´å¼ºåˆ¶åŠ ä¸Š "-"
        # ä¾‹å­ï¼šd43 -> D-43, D_43 -> D-43, d-43a -> D-43A
        standard_tag = f"{prefix.upper()}-{digits}{suffix.upper()}"
        normalized_tags.append(standard_tag)
    
    # è¿”å›å»é‡åçš„ç»“æœ
    return list(set(normalized_tags))

def clean_markdown(content):
    content = re.sub(r'^\\---', '---', content, flags=re.MULTILINE)
    content = re.sub(r'^\\(#+)', r'\1', content, flags=re.MULTILINE)
    content = re.sub(r'[\u200B-\u200D\uFEFF]', '', content)
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.match(r'^\|[\s*:|-]+\|$', line.strip()): continue 
        if line.strip().startswith('|'):
            line = line.strip('|').replace('|', '   ')
        cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)

def hierarchical_chunking(content, file_path):
    file_name = os.path.basename(file_path).replace('.md', '')
    post = frontmatter.loads(content)
    doc_metadata = post.metadata
    main_content = post.content
    doc_title = doc_metadata.get('title', file_name)
    final_chunks = []
    
    # è®¾å®šå‚æ•°
    MAX_CHUNK_LEN = 1200 
    OVERLAP_LEN = 200

    h3_blocks = re.split(r'(?=^###\s+)', main_content, flags=re.MULTILINE)
    for i, h3_block in enumerate(h3_blocks):
        h3_block = h3_block.strip()
        if not h3_block: continue
        
        h3_match = re.search(r'^###\s+(.*)$', h3_block, flags=re.MULTILINE)
        h3_title = h3_match.group(1).strip() if h3_match else "æ¦‚è§ˆ"
        h3_content = re.sub(r'^###\s+.*$', '', h3_block, flags=re.MULTILINE).strip()
        
        if '#### ' in h3_content:
            h4_blocks = re.split(r'(?=^####\s+)', h3_content, flags=re.MULTILINE)
            for j, h4_block in enumerate(h4_blocks):
                h4_block = h4_block.strip()
                if len(h4_block) < 20: continue
                
                h4_match = re.search(r'^####\s+(.*)$', h4_block, flags=re.MULTILINE)
                h4_title = h4_match.group(1).strip() if h4_match else ""
                content_body = re.sub(r'^####\s+.*$', '', h4_block, flags=re.MULTILINE).strip()
                
                breadcrumb = f"{doc_title} > {h3_title}" + (f" > {h4_title}" if h4_title else "")
                
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(content_body, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-{j}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
        else:
            if len(h3_content) > 20:
                breadcrumb = f"{doc_title} > {h3_title}"
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(h3_content, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
    return final_chunks

def analyze_intent_with_llm(prompt, extracted_tags):
    # å°†æ¨¡å‹æç¤ºè¯ä¹Ÿæ”¹ä¸ºä¸­æ–‡ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å‡†ç¡®åœ°æŒ‰ä¸­æ–‡é€»è¾‘æ€è€ƒ
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šæ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾å¹¶è¿”å› JSONã€‚
    å¯é€‰æ„å›¾ï¼š
    - Path_Analysis: è¯¢é—®ç‰©æ–™æµå‘ã€è·¯å¾„ã€ç»è¿‡å“ªé‡Œã€‚
    - Fault_Diagnosis: è¯¢é—®æ•…éšœåŸå› ã€ä¸Šæ¸¸æº¯æºã€‚
    - Status_Check: è¯¢é—®è®¾å¤‡è®¾è®¡å‚æ•°ã€ç›‘æ§ä»ªè¡¨ã€‚
    - Procedure_Query: è¯¢é—®æ“ä½œæ­¥éª¤ã€ç†ŸåŒ–æµç¨‹ã€‚
    - Info_Query: è¯¢é—®åŸºæœ¬å®šä¹‰æˆ–é€šç”¨ä¿¡æ¯ã€‚

    ç”¨æˆ·å·²æå–ä½å·ï¼š{extracted_tags}
    è¿”å›æ ¼å¼ï¼š{{"intent": "æ„å›¾åç§°", "start_node": "èµ·ç‚¹ä½å·", "end_node": "ç»ˆç‚¹ä½å·", "target_name": "è®¾å¤‡åç§°"}}"""
    try:
        # è¿™é‡Œå»ºè®®ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªèƒ½èŠå¤©çš„æ¨¡å‹
        response = ollama.chat(model=LLM_MODEL, messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': prompt}
        ], format='json')
        return json.loads(response['message']['content'])
    except:
        return {"intent": "Info_Query"}

def build_cypher(llm_result, extracted_tags, user_text):
    intent = llm_result.get("intent", "Info_Query")
    tags = extracted_tags
    
    # å¼ºåˆ¶ä¿®æ­£ï¼šå¦‚æœæ¶‰åŠä¸¤ä¸ªä½å·ä¸”åŒ…å«æµç¨‹åŠ¨è¯ï¼Œè®¾ä¸ºè·¯å¾„åˆ†æ
    if len(tags) >= 2 and any(k in user_text for k in ["åˆ°", "æµ", "ç»è¿‡", "å»å¾€", "æµç¨‹", "è”ç³»", "å·¥è‰º"]):
        intent = "Path_Analysis"
        
    cypher = ""; params = {}

    if intent == "Path_Analysis":
        start = llm_result.get("start_node") or (tags[0] if tags else None)
        end = llm_result.get("end_node") or (tags[1] if len(tags)>1 else None)
        
        if start and end:
            # è·¯å¾„åˆ†æï¼šè¿”å›æå…¶è¯¦å°½çš„èŠ‚ç‚¹å’Œç®¡é“å±æ€§
            cypher = """
            MATCH (start:Asset), (end:Asset)
    WHERE (start.Tag STARTS WITH $startTag OR replace(start.Tag, '-', '') = $startTagAlt)
      AND (end.Tag STARTS WITH $endTag OR replace(end.Tag, '-', '') = $endTagAlt)
    
    // 1. å¯»æ‰¾é¡ºæµæ–¹å‘çš„æœ€çŸ­è·¯å¾„
    MATCH path = shortestPath((start)-[:PIPE|MEASURES*..30]->(end))
    
    // 2. æ ¼å¼åŒ–è¿”å›ï¼šä¿ç•™æ‰€æœ‰ç‰©ç†è¯­ä¹‰å¹¶æŒ‰å·¥è‰ºé¡ºåºäº¤ç»‡
    RETURN 
        'Path_Analysis' as intent,
        [i IN range(0, length(path)-1) | {
            // èµ·ç‚¹è®¾å¤‡
            from_equipment: CASE 
                WHEN nodes(path)[i].Tag <> "TEE" AND nodes(path)[i].type <> "Instrument" AND nodes(path)[i].type <> "TappingPoint"
                THEN {tag: nodes(path)[i].Tag, desc: nodes(path)[i].desc, type: nodes(path)[i].type}
                ELSE "è¾…åŠ©è¿æ¥ç‚¹(TEE/æµ‹ç‚¹)" 
            END,
            
            // ç®¡é“è¯­ä¹‰ï¼ˆ12é¡¹å®Œæ•´å±æ€§ï¼‰
            pipeline_semantics: {
                fluid: relationships(path)[i].fluid,
                dn: relationships(path)[i].dn,
                material: relationships(path)[i].material,
                insulation: relationships(path)[i].insulation,
                pn: relationships(path)[i].pn,
                fromPort: relationships(path)[i].fromPort,
                toPort: relationships(path)[i].toPort,
                fromDesc: relationships(path)[i].fromDesc,
                toDesc: relationships(path)[i].toDesc,
                fromRegion: relationships(path)[i].fromRegion,
                toRegion: relationships(path)[i].toRegion,
                tag: relationships(path)[i].tag
            },
            
            // ç»ˆç‚¹è®¾å¤‡
            to_equipment: CASE 
                WHEN nodes(path)[i+1].Tag <> "TEE" AND nodes(path)[i+1].type <> "Instrument" AND nodes(path)[i+1].type <> "TappingPoint"
                THEN {tag: nodes(path)[i+1].Tag, desc: nodes(path)[i+1].desc, type: nodes(path)[i+1].type}
                ELSE "è¾…åŠ©è¿æ¥ç‚¹(TEE/æµ‹ç‚¹)" 
            END
        }] as structured_process_flow,
        length(path) as total_hops
            """
            params = {
                "startTag": start, "startTagAlt": start.replace("-", ""),
                "endTag": end, "endTagAlt": end.replace("-", "")
            }

    elif intent == "Fault_Diagnosis":
        # æ•…éšœè¯Šæ–­ï¼šä¾§é‡äºè¿½æº¯ä¸Šæ¸¸è®¾å¤‡åŠå…¶æè¿°
        cypher = """
        UNWIND $tags AS qTag
        MATCH (target:Asset) WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        OPTIONAL MATCH (target)<-[:PIPE*1..5]-(source:Asset)
        WHERE source.Tag <> 'TEE'
        RETURN target.Tag as tag, 'Fault_Diagnosis' as intent, 
               collect(DISTINCT {tag: source.Tag, desc: source.desc}) as upstream_trace
        """
        params = {"tags": tags}

    elif intent == "Status_Check":
        # ä»ªè¡¨æ£€æŸ¥ï¼šä¾§é‡äº MEASURES å…³ç³»å’Œ Instrument èŠ‚ç‚¹çš„å‚æ•°
        cypher = """
        UNWIND $tags AS qTag
        MATCH (target:Asset) WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        OPTIONAL MATCH (target)-[:MEASURES]-(sensor:Instrument)
        RETURN target.Tag as tag, target.desc as desc, 
               {temp: target.design_temp, press: target.design_press, spec: target.spec} as design_params,
               collect(DISTINCT {tag: sensor.Tag, desc: sensor.desc, range: sensor.range, unit: sensor.unit}) as sensors
        """
        params = {"tags": tags}

    else:
        # åŸºç¡€æŸ¥è¯¢ï¼šè¿”å›ä½å·ã€æè¿°å’Œç±»å‹
        cypher = """
        UNWIND $tags AS qTag
        MATCH (target:Asset) WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        RETURN target.Tag as tag, target.desc as name, target.type as category, 'Info_Query' as intent
        """
        params = {"tags": tags}

    return cypher, params

def query_neo4j(query, params):
    if not query: return []
    # ç»ˆç«¯è°ƒè¯•ä¿¡æ¯ä¿ç•™è‹±æ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    print(f"\n[è°ƒè¯•] æ‰§è¡Œ Cypher: {query}\n[è°ƒè¯•] å‚æ•°: {params}", file=sys.stderr, flush=True)
    try:
        with neo4j_driver.session() as session:
            result = session.run(query, **params)
            return [dict(record) for record in result]
    except Exception as e:
        print(f"[é”™è¯¯] Neo4j æŸ¥è¯¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
        return []

# ================= 4. Streamlit ç•Œé¢æ˜¾ç¤º (ä¸­æ–‡ä¸­æ–‡åŒ–) =================
st.set_page_config(page_title="åŒ–å·¥çŸ¥è¯†å›¾è°±", layout="wide", page_icon="ğŸ§ª")
st.title("ğŸ§ª åŒ–å·¥è£…ç½®å›¾è°± + æ–‡æ¡£å‘é‡æ··åˆçŸ¥è¯†åº“")

# --- ä¾§è¾¹æ ï¼šç®¡ç†é¢æ¿ ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç³»ç»Ÿåå°ç®¡ç†")
    
    # æ˜¾ç¤ºå‘é‡åº“ç»Ÿè®¡
    try:
        db_count = collection.count()
        st.metric("å·²å­˜å‚¨çŸ¥è¯†åˆ‡ç‰‡æ•°", db_count)
    except:
        st.error("å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    st.markdown("---")
    
    # çŸ¥è¯†åŒæ­¥åŠŸèƒ½
    st.subheader("ğŸ“ æ•°æ®åŒæ­¥")
    path_input = st.text_area("Markdown æ–‡æ¡£è·¯å¾„", "/Users/chenfeng/åŸ¹è®­èµ„æ–™/åŒ–å·¥è‹¯é…åŸºæœ¬çŸ¥è¯†/")
    
    if st.button("ğŸš€ å¼€å§‹å¢é‡åŒæ­¥"):
        all_chunks = []
        with st.spinner("æ­£åœ¨æ‰«æå¹¶è§£ææ–‡æ¡£..."):
            if os.path.exists(path_input):
                for root, _, files in os.walk(path_input):
                    for file in files:
                        if file.endswith('.md'):
                            try:
                                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                                    cleaned = clean_markdown(f.read())
                                    chunks = hierarchical_chunking(cleaned, os.path.join(root, file))
                                    all_chunks.extend(chunks)
                            except: pass
            
        if all_chunks:
            total = len(all_chunks)
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, chunk in enumerate(all_chunks):
                status_text.text(f"æ­£åœ¨å‘é‡åŒ– ({idx+1}/{total}): {chunk['id']}")
                try:
                    emb = ollama.embeddings(model=EMBED_MODEL, prompt=chunk['text'][:2000])['embedding']
                    collection.upsert(
                        ids=[chunk['id']], 
                        embeddings=[emb], 
                        documents=[chunk['text']], 
                        metadatas=[{k:str(v) for k,v in chunk['metadata'].items()}]
                    )
                except: pass
                progress_bar.progress((idx + 1) / total)
            
            status_text.text("âœ… åŒæ­¥åœ†æ»¡å®Œæˆï¼")
            st.balloons()
            st.rerun()
        else:
            st.warning("æœªåœ¨è¯¥è·¯å¾„ä¸‹æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")

    if st.button("ğŸ—‘ï¸ å±é™©æ“ä½œï¼šæ¸…ç©ºå‘é‡åº“"):
        chroma_client.delete_collection(COLLECTION_NAME)
        collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
        st.warning("åº“å·²æ¸…ç©º")
        st.rerun()

# --- å¯¹è¯åŒºåŸŸ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥
# --- å¯¹è¯åŒºåŸŸ (ä¼˜åŒ–ç‰ˆ) ---
if prompt := st.chat_input("æ‚¨å¯ä»¥é—®æˆ‘ï¼šD-14 ååº”å™¨çš„è®¾è®¡å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 1. æ£€ç´¢é˜¶æ®µ (ChromaDB + Neo4j)
        graph_data = []
        vector_docs = []
        
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢åŒåº“äº‹å®...", expanded=True) as status:
            extracted_tags = extract_tags(prompt)
            st.write(f"ğŸ·ï¸ **è¯†åˆ«ä½å·**: `{', '.join(extracted_tags) if extracted_tags else 'æœªè¯†åˆ«'}`")
            
            intent_res = analyze_intent_with_llm(prompt, extracted_tags)
            st.write(f"ğŸ¯ **è§£ææ„å›¾**: `{intent_res.get('intent', 'Info_Query')}`")
            
            cypher, params = build_cypher(intent_res, extracted_tags, prompt)
            if cypher:
                graph_data = query_neo4j(cypher, params)
                # --- è¿™é‡Œçš„é€»è¾‘æ”¹ä¸ºæ¡ä»¶æ˜¾ç¤º ---
                if graph_data:
                        st.write("âœ… **å›¾è°±äº‹å®**: å·²æˆåŠŸæ£€ç´¢åˆ°å…³è”æ‹“æ‰‘")
                else:
                     st.write("âš ï¸ **å›¾è°±äº‹å®**: æœªèƒ½åœ¨å›¾æ•°æ®åº“ä¸­æ‰¾åˆ°åŒ¹é…çš„è·¯å¾„æˆ–èŠ‚ç‚¹")
            
            q_emb = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)['embedding']
            vector_res = collection.query(query_embeddings=[q_emb], n_results=3)
            vector_docs = vector_res['documents'][0]
            st.write(f"ğŸ“„ **æ–‡æ¡£çŸ¥è¯†**: å·²åŒ¹é…ç›¸å…³æè¿°ç‰‡æ®µ")
            
            status.update(label=f"âœ… æ£€ç´¢å®Œæˆ: å‘½ä¸­ {len(graph_data)} æ¡äº‹å®, {len(vector_docs)} æ®µæ–‡æ¡£", state="complete", expanded=False)

        # 2. å›ç­”ç”Ÿæˆé˜¶æ®µ
        full_response = ""
        
        # --- ã€åŠ¨æ€å›¾æ ‡æ–¹æ¡ˆã€‘åˆ›å»ºä¸€ä¸ªåŒ–å­¦/AI åŠ¨æ€å ä½ç¬¦ ---
        thinking_container = st.empty()
        
        with thinking_container.container():
            st.markdown(
                """
                <style>
                @keyframes pulse-ring {
                  0% { transform: scale(.33); }
                  80%, 100% { opacity: 0; }
                }
                @keyframes pulse-dot {
                  0% { transform: scale(.8); }
                  50% { transform: scale(1); }
                  100% { transform: scale(.8); }
                }
                .ai-thinking-container {
                  display: flex;
                  flex-direction: column;
                  align-items: center;
                  justify-content: center;
                  padding: 20px;
                }
                .pulse-wrapper {
                  position: relative;
                  width: 60px;
                  height: 60px;
                }
                .pulse-dot {
                  position: absolute;
                  top: 15px; left: 15px;
                  width: 30px; height: 30px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-dot 1.25s cubic-bezier(0.455, 0.03, 0.515, 0.955) -.4s infinite;
                  display: flex;
                  align-items: center;
                  justify-content: center;
                  z-index: 2;
                }
                .pulse-ring {
                  position: absolute;
                  top: 0; left: 0;
                  width: 60px; height: 60px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-ring 1.25s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
                  z-index: 1;
                }
                .thinking-text {
                  margin-top: 15px;
                  font-family: sans-serif;
                  color: #007bff;
                  font-weight: bold;
                  letter-spacing: 1px;
                }
                </style>
                
                <div class="ai-thinking-container">
                    <div class="pulse-wrapper">
                        <div class="pulse-ring"></div>
                        <div class="pulse-dot">
                            <span style="font-size: 18px;">ğŸ§ª</span>
                        </div>
                    </div>
                    <div class="thinking-text">åŠªåŠ›åˆ†æä¸­...</div>
                </div>
                """, 
                unsafe_allow_html=True
            )


        # æœ€ç»ˆå›ç­”æ‰“å­—æœºæ˜¾ç¤ºçš„å ä½ç¬¦
        response_placeholder = st.empty()
        
        if not graph_data and not vector_docs:
            thinking_container.empty() # æ²¡æ‰¾åˆ°æ•°æ®ï¼Œç›´æ¥æ¸…é™¤æç¤º
            response_placeholder.warning("âš ï¸ æ ¹æ®ç›®å‰çŸ¥è¯†åº“è®°å½•ï¼Œæœªæ‰¾åˆ°ä¸è¯¥æé—®ç›¸å…³çš„ä½å·äº‹å®æˆ–æ–‡æ¡£è¯´æ˜ã€‚")
        else:
            # æ„é€ ä¸Šä¸‹æ–‡å’Œ Prompt
            h_context = f"ã€å›¾è°±äº‹å®ã€‘: {json.dumps(graph_data, ensure_ascii=False)}\n\nã€æ–‡æ¡£èµ„æ–™ã€‘: {' '.join(vector_docs)}"
            
            # --- æç¤ºè¯å¾®è°ƒ (ç¡®ä¿æ¨¡å‹ä¸ä¼šå¤ªå•°å—¦) ---
            sys_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ–å·¥è£…ç½®ä¸“å®¶ã€‚è¯·ç»“åˆã€å›¾è°±äº‹å®ã€‘å’Œã€æ–‡æ¡£èµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚å¦‚æœã€å›¾è°±äº‹å®ã€‘å’Œã€çŸ¥è¯†åº“æ–‡æ¡£ã€‘ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå°±ç›´æ¥è¯´'æ ¹æ®æˆ‘ç°æœ‰çš„çŸ¥è¯†ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜'ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
            
             
            ### å›ç­”ç­–ç•¥
                        1. **ç»¼åˆåˆ¤æ–­**: å›¾è°±æä¾›äº†å‡†ç¡®çš„è®¾å¤‡ä½å·åŠŸèƒ½æè¿°å’Œè¿æ¥å…³ç³»çš„æ¥æºå»å‘ç­‰ï¼ŒçŸ¥è¯†åº“æä¾›äº†è¯¦ç»†çš„æ“ä½œæ­¥éª¤å’ŒåŸç†ã€‚
                        2 . **æ•…éšœè¯Šæ–­**: å¦‚æœå›¾è°±æ˜¾ç¤ºå¤šæ¡ä¾›æ–™æ”¯è·¯ï¼Œè¯·åˆ†åˆ«åˆ†æã€‚ç»“åˆçŸ¥è¯†åº“ä¸­çš„æ•…éšœå¤„ç†æ–¹æ³•ã€‚
                        3. **å†²çªå¤„ç†**: æ¶‰åŠè®¾å¤‡è¿æ¥å…³ç³»æ—¶ï¼Œä»¥å›¾è°±ä¸ºå‡†ï¼›æ¶‰åŠæ“ä½œç»†èŠ‚æ—¶ï¼Œä»¥çŸ¥è¯†åº“ä¸ºå‡†ã€‚

            
            """

            try:
                # è°ƒç”¨æ¨¡å‹
                stream = ollama.chat(model=LLM_MODEL, messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': f"äº‹å®èƒŒæ™¯: {h_context}\né—®é¢˜: {prompt}"}
                ], stream=True)

                # --- æ ¸å¿ƒæ”¹è¿›ï¼šç²¾ç¡®æ§åˆ¶æ¸…é™¤æ—¶æœº ---
                for chunk in stream:
                    content = chunk['message']['content']
                    
                    # åªæœ‰å½“æ¨¡å‹çœŸæ­£è¾“å‡ºäº†å†…å®¹ï¼ˆä¸”éç©ºï¼‰æ—¶ï¼Œæ‰æ¸…é™¤â€œæ€è€ƒä¸­â€æç¤º
                    if content.strip() and not full_response:
                        thinking_container.empty()
                    
                    full_response += content
                    # åŠ¨æ€å±•ç¤ºæ‰“å­—æœºæ•ˆæœ
                    response_placeholder.markdown(full_response + "â–Œ")
                
                # å®Œæˆè¾“å‡ºï¼Œç§»é™¤å…‰æ ‡
                response_placeholder.markdown(full_response)
                
            except Exception as e:
                thinking_container.empty()
                st.error(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")

        # 3. è¯æ®æº¯æºæ˜¾ç¤º (ä¿æŒä¸å˜)
        if graph_data or vector_docs:
            with st.expander("ğŸ” åŸå§‹æ£€ç´¢è¯æ®"):
                tab1, tab2 = st.tabs(["å›¾è°±äº‹å®", "æ–‡æ¡£ç‰‡æ®µ"])
                with tab1: st.json(graph_data)
                with tab2:
                    for d in vector_docs: st.info(d)

    # å°†åŠ©æ‰‹çš„å›ç­”å­˜å…¥å¯¹è¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})