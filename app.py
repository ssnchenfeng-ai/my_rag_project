import streamlit as st
import sys
from dotenv import load_dotenv
import os
import re
import json
import ollama
import chromadb
import frontmatter
from neo4j import GraphDatabase

# ================= 0. åŠ è½½ç¯å¢ƒå˜é‡ =================
load_dotenv()  # è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶

# ================= 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½® =================
# ä½¿ç”¨ os.getenv('å˜é‡å', 'é»˜è®¤å€¼') çš„æ–¹å¼è¯»å–
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")  # å¯†ç ä¸å»ºè®®è®¾é»˜è®¤å€¼

CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chemical_kb")

EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text:latest")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-r1:latest")

# è·å–æœ¬åœ°æ•°æ®è·¯å¾„
DEFAULT_DATA_PATH = os.getenv("DATA_PATH", "./data/")

# ================= 2. åˆå§‹åŒ–è¿æ¥ =================
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
# --- åœ¨è¿™é‡Œæ’å…¥ã€é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°ã€‘ ---
def recursive_split_text(text, max_chars=1200, overlap=200):
    """å½“æ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰ä¼˜å…ˆçº§è¿›è¡Œé€’å½’æ‹†åˆ†"""
    if len(text) <= max_chars:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunk = text[start:end]
        if end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œæˆ–å¥å·ï¼Œé¿å…ç”Ÿç¡¬åˆ‡æ–­
            last_break = max(chunk.rfind('\n'), chunk.rfind('ã€‚'), chunk.rfind('. '))
            if last_break > max_chars * 0.5: 
                end = start + last_break + 1
                chunk = text[start:end]
        chunks.append(chunk)
        start += (len(chunk) - overlap)
        if len(chunk) <= overlap: break
    return chunks

def extract_tags(text):
    # 1. åŒ¹é…æ¨¡å¼ï¼š
    # ([a-zA-Z]{1,3})  -> æ•è·1-3ä½å­—æ¯å‰ç¼€ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # [-_]?            -> åŒ¹é…å¯é€‰çš„è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿
    # (\d{2,4})        -> æ•è·2-4ä½æ•°å­—
    # ([a-zA-Z]?)      -> æ•è·å¯é€‰çš„ä¸€ä½å­—æ¯åç¼€
    pattern = r'([a-zA-Z]{1,3})[\s\-_]?(\d{2,4})([a-zA-Z]?)'
    # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç»„åˆ
    matches = re.findall(pattern, text)
    
    normalized_tags = []
    for prefix, digits, suffix in matches:
        # 2. æ ‡å‡†åŒ–å¤„ç†ï¼š
        # å…¨éƒ¨è½¬ä¸ºå¤§å†™ï¼Œå¹¶åœ¨å­—æ¯ä¸æ•°å­—ä¹‹é—´å¼ºåˆ¶åŠ ä¸Š "-"
        # ä¾‹å­ï¼šd43 -> D-43, D_43 -> D-43, d-43a -> D-43A
        standard_tag = f"{prefix.upper()}-{digits}{suffix.upper()}"
        normalized_tags.append(standard_tag)
    
    # è¿”å›å»é‡åçš„ç»“æœ
    return list(set(normalized_tags))
def is_noise_node(node_labels, node_tag):
    """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦å¿½ç•¥çš„è¾…åŠ©èŠ‚ç‚¹ï¼ˆå¦‚ä¸‰é€šã€è·¨é¡µç¬¦ã€ä»ªè¡¨ï¼‰"""
    ignore_labels = ['OffPageConnector', 'Instrument', 'Drawing']
    ignore_tags = ['TEE', 'TappingPoint']
    # æ£€æŸ¥æ ‡ç­¾æˆ–ä½å·æ˜¯å¦åŒ…å«å¿½ç•¥å…³é”®è¯
    if any(l in node_labels for l in ignore_labels): return True
    if any(t in str(node_tag).upper() for t in ignore_tags): return True
    return False

def clean_markdown(content):
    content = re.sub(r'^\\---', '---', content, flags=re.MULTILINE)
    content = re.sub(r'^\\(#+)', r'\1', content, flags=re.MULTILINE)
    content = re.sub(r'[\u200B-\u200D\uFEFF]', '', content)
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.match(r'^\|[\s*:|-]+\|$', line.strip()): continue 
        if line.strip().startswith('|'):
            line = line.strip('|').replace('|', '   ')
        cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)

def hierarchical_chunking(content, file_path):
    file_name = os.path.basename(file_path).replace('.md', '')
    post = frontmatter.loads(content)
    doc_metadata = post.metadata
    main_content = post.content
    doc_title = doc_metadata.get('title', file_name)
    final_chunks = []
    
    # è®¾å®šå‚æ•°
    MAX_CHUNK_LEN = 1200 
    OVERLAP_LEN = 200

    h3_blocks = re.split(r'(?=^###\s+)', main_content, flags=re.MULTILINE)
    for i, h3_block in enumerate(h3_blocks):
        h3_block = h3_block.strip()
        if not h3_block: continue
        
        h3_match = re.search(r'^###\s+(.*)$', h3_block, flags=re.MULTILINE)
        h3_title = h3_match.group(1).strip() if h3_match else "æ¦‚è§ˆ"
        h3_content = re.sub(r'^###\s+.*$', '', h3_block, flags=re.MULTILINE).strip()
        
        if '#### ' in h3_content:
            h4_blocks = re.split(r'(?=^####\s+)', h3_content, flags=re.MULTILINE)
            for j, h4_block in enumerate(h4_blocks):
                h4_block = h4_block.strip()
                if len(h4_block) < 20: continue
                
                h4_match = re.search(r'^####\s+(.*)$', h4_block, flags=re.MULTILINE)
                h4_title = h4_match.group(1).strip() if h4_match else ""
                content_body = re.sub(r'^####\s+.*$', '', h4_block, flags=re.MULTILINE).strip()
                
                breadcrumb = f"{doc_title} > {h3_title}" + (f" > {h4_title}" if h4_title else "")
                
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(content_body, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-{j}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
        else:
            if len(h3_content) > 20:
                breadcrumb = f"{doc_title} > {h3_title}"
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(h3_content, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
    return final_chunks

def analyze_intent_with_llm(prompt, extracted_tags):
    # å°†æ¨¡å‹æç¤ºè¯ä¹Ÿæ”¹ä¸ºä¸­æ–‡ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å‡†ç¡®åœ°æŒ‰ä¸­æ–‡é€»è¾‘æ€è€ƒ
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šä¸“å®¶çº§æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾å¹¶è¿”å› JSONã€‚
    
    æ„å›¾åˆ†ç±»æ ‡å‡†ï¼š
    1. Path_Analysis: è¯¢é—®å·¥è‰ºæµç¨‹ã€ç‰©æ–™æµå‘ã€è·¯å¾„ã€ç»è¿‡å“ªäº›è®¾å¤‡ã€è·¨é¡µæµç¨‹ã€‚
    2. Fault_Diagnosis: è¯¢é—®æ•…éšœåŸå› ã€ä¸Šæ¸¸æº¯æºã€ä¸²æ–™åˆ†æã€å¼‚å¸¸æ³¢åŠ¨æ¥æºã€‚
    3. Status_Check: è¯¢é—®è®¾å¤‡è®¾è®¡å‚æ•°ï¼ˆå‹åŠ›/æ¸©åº¦/æè´¨ï¼‰ã€ç›‘æ§ä»ªè¡¨ä½å·ã€é‡ç¨‹ã€‚
    4. Procedure_Query: è¯¢é—®æ“ä½œæ­¥éª¤ã€å¯åŠ¨/åœæ­¢é¡ºåºã€å®‰å…¨æ³¨æ„äº‹é¡¹ã€SOPã€‚
    5. Info_Query: è¯¢é—®åŸºæœ¬å®šä¹‰ã€æœ¯è¯­è§£é‡Šã€é€šç”¨å¸¸è¯†ã€‚

    ç”¨æˆ·æå–ä½å·ï¼š{extracted_tags}
    è¿”å›æ ¼å¼ï¼š{{"intent": "æ„å›¾åç§°", "start_node": "èµ·ç‚¹ä½å·", "end_node": "ç»ˆç‚¹ä½å·", "target_tag": "ç›®æ ‡ä½å·"}}"""
    
    try:
        response = ollama.chat(model=LLM_MODEL, messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': prompt}
        ], format='json')
        return json.loads(response['message']['content'])
    except:
        return {"intent": "Info_Query"}

def build_cypher(llm_result, extracted_tags, user_text):
    intent = llm_result.get("intent", "Info_Query")
    tags = extracted_tags
    
    # è·¯å¾„åˆ†æå¼ºåˆ¶ä¿®æ­£
    if len(tags) >= 2 and any(k in user_text for k in ["åˆ°", "æµ", "ç»è¿‡", "å»å¾€", "æµç¨‹"]):
        intent = "Path_Analysis"
        
    cypher = ""; params = {}

    if intent == "Path_Analysis":
        # ... (ä¿æŒä¹‹å‰ä¼˜åŒ–çš„è·¨é¡µè·¯å¾„ä»£ç ) ...
        start = llm_result.get("start_node") or (tags[0] if tags else None)
        end = llm_result.get("end_node") or (tags[1] if len(tags)>1 else None)
        if start and end:
            cypher = """
            MATCH (start:Asset), (end:Asset)
            WHERE (start.Tag STARTS WITH $startTag OR replace(start.Tag, '-', '') = $startTagAlt)
              AND (end.Tag STARTS WITH $endTag OR replace(end.Tag, '-', '') = $endTagAlt)
            MATCH path = shortestPath((start)-[:PIPE|LINKS_TO*..60]->(end))
            RETURN 'Path_Analysis' as intent,
                   [n in nodes(path) | {tag: n.Tag, desc: n.desc, labels: labels(n)}] as nodes_data,
                   [r in relationships(path) | properties(r)] as rels_data,
                   length(path) as total_hops
            """
            params = {"startTag": start, "startTagAlt": start.replace("-", ""), "endTag": end, "endTagAlt": end.replace("-", "")}

    elif intent == "Status_Check":
        # ã€æ–°å¢ã€‘çŠ¶æ€æ£€æŸ¥ï¼šæŸ¥è¯¢è®¾å¤‡å±æ€§ + å…³è”ä»ªè¡¨
        cypher = """
        UNWIND $tags AS qTag
        MATCH (e:Asset) WHERE e.Tag = qTag OR replace(e.Tag, '-', '') = replace(qTag, '-', '')
        OPTIONAL MATCH (i:Instrument)-[:MEASURES]->(e)
        RETURN 'Status_Check' as intent, e.Tag as tag, properties(e) as params,
               collect({tag: i.Tag, desc: i.desc, unit: i.unit, range: i.range}) as sensors
        """
        params = {"tags": tags}

    elif intent == "Fault_Diagnosis":
        # ... (ä¿æŒä¹‹å‰ä¼˜åŒ–çš„æº¯æºä»£ç ) ...
        cypher = """
        UNWIND $tags AS qTag
        MATCH (target:Asset) WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        MATCH path = (target)<-[:PIPE|LINKS_TO*1..6]-(source:Asset)
        WHERE NOT source:OffPageConnector AND source.Tag <> 'TEE'
        WITH target, source, relationships(path)[0] as r
        RETURN 'Fault_Diagnosis' as intent, target.Tag as tag,
               collect(DISTINCT {
                   source: source.Tag, 
                   s_desc: source.desc, 
                   fluid: r.fluid,
                   pipe_desc: r.desc  // <--- æ–°å¢ï¼šæŠ“å–ç®¡çº¿æè¿°
               }) as upstream_trace
        """
        params = {"tags": tags}

    elif intent == "Procedure_Query":
        # ã€æ–°å¢ã€‘è§„ç¨‹æŸ¥è¯¢ï¼šå›¾æ•°æ®åº“ä»…ç”¨äºç¡®è®¤ä½å·æè¿°ï¼Œä¸»è¦é å‘é‡åº“
        cypher = """
        UNWIND $tags AS qTag
        MATCH (e:Asset) WHERE e.Tag = qTag OR replace(e.Tag, '-', '') = replace(qTag, '-', '')
        RETURN 'Procedure_Query' as intent, e.Tag as tag, e.desc as desc
        """
        params = {"tags": tags}

    else: # Info_Query
        # ... (ä¿æŒåŸºç¡€æŸ¥è¯¢ä»£ç ) ...
        cypher = """
        UNWIND $tags AS qTag
        MATCH (e:Asset) WHERE e.Tag = qTag OR replace(e.Tag, '-', '') = replace(qTag, '-', '')
        OPTIONAL MATCH (e)-[r:PIPE|CONTROLS|MEASURES]-(neighbor:Asset)
        RETURN 'Info_Query' as intent, e.Tag as tag, e.desc as desc, e.type as type,
               collect(DISTINCT {
                   type: type(r), 
                   neighbor: neighbor.Tag, 
                   n_desc: neighbor.desc,
                   fluid: r.fluid, 
                   rel_desc: r.desc,   // <--- æ–°å¢ï¼šæŠ“å–å…³ç³»æè¿°
                   from_reg: r.fromRegion, 
                   to_reg: r.toRegion
               }) as topology
        """
        params = {"tags": tags}

    return cypher, params

def query_neo4j(query, params):
    if not query: return []
    # ç»ˆç«¯è°ƒè¯•ä¿¡æ¯ä¿ç•™è‹±æ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    print(f"\n[è°ƒè¯•] æ‰§è¡Œ Cypher: {query}\n[è°ƒè¯•] å‚æ•°: {params}", file=sys.stderr, flush=True)
    try:
        with neo4j_driver.session() as session:
            result = session.run(query, **params)
            return [dict(record) for record in result]
    except Exception as e:
        print(f"[é”™è¯¯] Neo4j æŸ¥è¯¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
        return []
# ==============================================================================
# ğŸ‘‡ğŸ‘‡ğŸ‘‡ è¯·åœ¨è¿™é‡Œæ’å…¥æ–°å¢çš„è¾…åŠ©å‡½æ•° ğŸ‘‡ğŸ‘‡ğŸ‘‡
# ==============================================================================

def translate_region(region_code):
    """å°†è‹±æ–‡åŒºåŸŸä»£ç ç¿»è¯‘ä¸ºä¸­æ–‡è¯­ä¹‰"""
    if not region_code: return "é€šç”¨æ¥å£"
    mapping = {
        'ShellSide': 'å£³ç¨‹',
        'ShellSide:Vapor': 'å£³ç¨‹(æ°”ç›¸)',
        'ShellSide:Liquid': 'å£³ç¨‹(æ¶²ç›¸)',
        'TubeSide': 'ç®¡ç¨‹',
        'TubeSide:Liquid': 'ç®¡ç¨‹(æ¶²ç›¸)',
        'TubeSide:Vapor': 'ç®¡ç¨‹(æ°”ç›¸)',
        'Jacket': 'å¤¹å¥—',
        'InnerVessel': 'å†…èƒ†',
        'ControlSignal': 'æ§åˆ¶ä¿¡å·æ¥å£',
        'UpperSaltChannel': 'ä¸Šç›é“',
        'LowerSaltChannel': 'ä¸‹ç›é“'
    }
    return mapping.get(region_code, region_code)

def format_graph_data(data, intent):
    """è¯­ä¹‰æ— æŸçš„è·¯å¾„å‹ç¼©ä¸å±æ€§èšåˆç®—æ³•"""
    if not data: return "æœªæŸ¥è¯¢åˆ°ç›¸å…³å›¾è°±äº‹å®ã€‚"
    text_lines = []

    # --- åœºæ™¯ A: çŠ¶æ€æ£€æŸ¥ ---
    if intent == "Status_Check":
        for record in data:
            text_lines.append(f"ğŸ“‹ **è®¾å¤‡å‚æ•°æ¡£æ¡ˆ**: {record['tag']}")
            params = record.get('params', {})
            for k, v in params.items():
                if k not in ['Tag', 'desc', 'id'] and v and v != "None":
                    text_lines.append(f"   - {k}: {v}")
            sensors = record.get('sensors', [])
            if sensors:
                text_lines.append(f"   **å…³è”ç›‘æ§ä»ªè¡¨**:")
                for s in sensors:
                    text_lines.append(f"   - ğŸ·ï¸ {s['tag']} ({s['desc']}) | é‡ç¨‹: {s.get('range','--')} {s.get('unit','')}")
            text_lines.append("")

    # --- åœºæ™¯ B: è·¯å¾„åˆ†æ (æ ¸å¿ƒèšåˆé€»è¾‘) ---
    elif intent == "Path_Analysis":
        for record in data:
            nodes = record.get('nodes_data', [])
            rels = record.get('rels_data', [])
            if not nodes: continue
            
            text_lines.append(f"ğŸ›£ï¸ **å…¨é“¾è·¯å·¥è‰ºè¿½è¸ª (è·¨åº¦: {record.get('total_hops', 0)} æ­¥)**:")
            current_equip = nodes[0]
            attr_accumulator = {} 

            for i in range(len(rels)):
                rel = rels[i]
                next_node = nodes[i+1]
                # ç´¯ç§¯ç®¡çº¿å±æ€§
                for k, v in rel.items():
                    if v and v != "None": attr_accumulator[k] = v
                
                # é‡åˆ°çœŸå®è®¾å¤‡æ‰è¾“å‡º
                if not is_noise_node(next_node.get('labels', []), next_node.get('tag', '')):
                    src_str = f"**{current_equip['tag']}** ({current_equip.get('desc','è®¾å¤‡')})"
                    tgt_str = f"**{next_node['tag']}** ({next_node.get('desc','è®¾å¤‡')})"
                    
                    fluid = attr_accumulator.get('fluid', 'æœªçŸ¥ä»‹è´¨')
                    dn = f"{attr_accumulator['dn']}" if attr_accumulator.get('dn') else ""
                    mat = attr_accumulator.get('material', '')
                    p_desc = attr_accumulator.get('desc', '') # è·å–ç®¡çº¿æè¿°
                    
                    from_reg = translate_region(rels[i - (i if i==0 else 0)].get('fromRegion'))
                    to_reg = translate_region(rel.get('toRegion'))

                    # ã€ä¿®æ­£ç‚¹ã€‘ï¼šå°† p_desc æ”¾å…¥ join åˆ—è¡¨ä¸­
                    pipe_detail = " | ".join(filter(None, [fluid, dn, mat, p_desc])) 
                    line = f"   ğŸ“ {src_str} `[{from_reg}]` ==( ğŸŒŠ {pipe_detail} )==> `[{to_reg}]` {tgt_str}"
                    text_lines.append(line)
                    
                    current_equip = next_node
                    attr_accumulator = {}
            text_lines.append("*(æ³¨ï¼šå·²è‡ªåŠ¨åˆå¹¶è·¨é¡µè¿æ¥ç¬¦åŠä¸‰é€šèŠ‚ç‚¹çš„ç‰©ç†å±æ€§)*\n")

    # --- åœºæ™¯ C: æ•…éšœè¯Šæ–­ ---
    elif intent == "Fault_Diagnosis":
        for record in data:
            text_lines.append(f"ğŸ› ï¸ **æ•…éšœæº¯æºç›®æ ‡**: {record['tag']}")
            for trace in record.get('upstream_trace', []):
                # ã€ä¿®æ­£ç‚¹ã€‘ï¼šå¢åŠ ç®¡çº¿æè¿°å±•ç¤º
                p_desc = f"[{trace['pipe_desc']}]" if trace.get('pipe_desc') else ""
                text_lines.append(f"   â¬†ï¸ æ¥æº: **{trace['source']}**({trace.get('s_desc','è®¾å¤‡')}) --({trace.get('fluid','ä»‹è´¨')}{p_desc})--> è¿›å…¥ç›®æ ‡")

    # --- åœºæ™¯ D: è§„ç¨‹æŸ¥è¯¢ ---
    elif intent == "Procedure_Query":
        for record in data:
            text_lines.append(f"ğŸ“– **æ­£åœ¨æ£€ç´¢å…³äº {record['tag']}({record['desc']}) çš„æ“ä½œè§„ç¨‹...**")

    # --- åœºæ™¯ E: åŸºç¡€æŸ¥è¯¢ ---
    else:
        for record in data:
            text_lines.append(f"â„¹ï¸ **è®¾å¤‡æ¡£æ¡ˆ**: {record.get('tag')} ({record.get('desc', 'æ— æè¿°')})")
            # ã€ä¿®æ­£ç‚¹ã€‘ï¼šåŸºç¡€æŸ¥è¯¢ä¹Ÿåº”è¯¥å±•ç¤ºå‘¨å›´çš„æ‹“æ‰‘å…³ç³»
            for t in record.get('topology', []):
                p_desc = f"[{t['rel_desc']}]" if t.get('rel_desc') else ""
                reg = translate_region(t.get('from_reg') or t.get('to_reg'))
                text_lines.append(f"   - å…³è”: **{t['neighbor']}**({t.get('n_desc','è®¾å¤‡')}) | {t['fluid']} {p_desc} | æ¥å£: {reg}")

    return "\n".join(text_lines)
# ==============================================================================
# ğŸ‘†ğŸ‘†ğŸ‘† æ’å…¥ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
# ============================================================================

# ================= 4. Streamlit ç•Œé¢æ˜¾ç¤º (ä¸­æ–‡ä¸­æ–‡åŒ–) =================
st.set_page_config(page_title="åŒ–å·¥çŸ¥è¯†å›¾è°±", layout="wide", page_icon="ğŸ§ª")
st.title("ğŸ§ª åŒ–å·¥è£…ç½®å›¾è°± + æ–‡æ¡£å‘é‡æ··åˆçŸ¥è¯†åº“")

# --- ä¾§è¾¹æ ï¼šç®¡ç†é¢æ¿ ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç³»ç»Ÿåå°ç®¡ç†")
    
    # æ˜¾ç¤ºå‘é‡åº“ç»Ÿè®¡
    try:
        db_count = collection.count()
        st.metric("å·²å­˜å‚¨çŸ¥è¯†åˆ‡ç‰‡æ•°", db_count)
    except:
        st.error("å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    st.markdown("---")
    
    # çŸ¥è¯†åŒæ­¥åŠŸèƒ½
    st.subheader("ğŸ“ æ•°æ®åŒæ­¥")
    path_input = st.text_area("Markdown æ–‡æ¡£è·¯å¾„", "/Users/chenfeng/åŸ¹è®­èµ„æ–™/åŒ–å·¥è‹¯é…åŸºæœ¬çŸ¥è¯†/")
    
    if st.button("ğŸš€ å¼€å§‹å¢é‡åŒæ­¥"):
        all_chunks = []
        with st.spinner("æ­£åœ¨æ‰«æå¹¶è§£ææ–‡æ¡£..."):
            if os.path.exists(path_input):
                for root, _, files in os.walk(path_input):
                    for file in files:
                        if file.endswith('.md'):
                            try:
                                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                                    cleaned = clean_markdown(f.read())
                                    chunks = hierarchical_chunking(cleaned, os.path.join(root, file))
                                    all_chunks.extend(chunks)
                            except: pass
            
        if all_chunks:
            total = len(all_chunks)
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, chunk in enumerate(all_chunks):
                status_text.text(f"æ­£åœ¨å‘é‡åŒ– ({idx+1}/{total}): {chunk['id']}")
                try:
                    emb = ollama.embeddings(model=EMBED_MODEL, prompt=chunk['text'][:2000])['embedding']
                    collection.upsert(
                        ids=[chunk['id']], 
                        embeddings=[emb], 
                        documents=[chunk['text']], 
                        metadatas=[{k:str(v) for k,v in chunk['metadata'].items()}]
                    )
                except: pass
                progress_bar.progress((idx + 1) / total)
            
            status_text.text("âœ… åŒæ­¥åœ†æ»¡å®Œæˆï¼")
            st.balloons()
            st.rerun()
        else:
            st.warning("æœªåœ¨è¯¥è·¯å¾„ä¸‹æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")

    if st.button("ğŸ—‘ï¸ å±é™©æ“ä½œï¼šæ¸…ç©ºå‘é‡åº“"):
        chroma_client.delete_collection(COLLECTION_NAME)
        collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
        st.warning("åº“å·²æ¸…ç©º")
        st.rerun()

# --- å¯¹è¯åŒºåŸŸ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥
# --- å¯¹è¯åŒºåŸŸ (ä¼˜åŒ–ç‰ˆ) ---
if prompt := st.chat_input("æ‚¨å¯ä»¥é—®æˆ‘ï¼šD-14 ååº”å™¨çš„è®¾è®¡å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 1. æ£€ç´¢é˜¶æ®µ (ChromaDB + Neo4j)
        graph_data = []
        vector_docs = []
        
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢åŒåº“äº‹å®...", expanded=True) as status:
            extracted_tags = extract_tags(prompt)
            # ã€ä¿®æ­£ç‚¹ 1ã€‘ï¼šå…ˆè¯†åˆ«æ„å›¾
            intent_res = analyze_intent_with_llm(prompt, extracted_tags)
            current_intent = intent_res.get('intent', 'Info_Query') # è·å–çœŸå®æ„å›¾
            st.write(f"ğŸ·ï¸ **è¯†åˆ«ä½å·**: `{', '.join(extracted_tags)}` | ğŸ¯ **æ„å›¾**: `{current_intent}`")
            
            # ã€ä¿®æ­£ç‚¹ 2ã€‘ï¼šæ ¹æ®çœŸå®æ„å›¾å†³å®šæ£€ç´¢æ–‡æ¡£æ•°
            n_docs = 6 if current_intent == "Procedure_Query" else 3
            
            cypher, params = build_cypher(intent_res, extracted_tags, prompt)
            if cypher:
                graph_data = query_neo4j(cypher, params)
            
            q_emb = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)['embedding']
            vector_res = collection.query(query_embeddings=[q_emb], n_results=n_docs)
            vector_docs = vector_res['documents'][0]
            
            status.update(label=f"âœ… æ£€ç´¢å®Œæˆ: {current_intent}", state="complete", expanded=False)

        # 2. å›ç­”ç”Ÿæˆé˜¶æ®µ
        full_response = ""
        
        # --- ã€åŠ¨æ€å›¾æ ‡æ–¹æ¡ˆã€‘åˆ›å»ºä¸€ä¸ªåŒ–å­¦/AI åŠ¨æ€å ä½ç¬¦ ---
        thinking_container = st.empty()
        
        with thinking_container.container():
            st.markdown(
                """
                <style>
                @keyframes pulse-ring {
                  0% { transform: scale(.33); }
                  80%, 100% { opacity: 0; }
                }
                @keyframes pulse-dot {
                  0% { transform: scale(.8); }
                  50% { transform: scale(1); }
                  100% { transform: scale(.8); }
                }
                .ai-thinking-container {
                  display: flex;
                  flex-direction: column;
                  align-items: center;
                  justify-content: center;
                  padding: 20px;
                }
                .pulse-wrapper {
                  position: relative;
                  width: 60px;
                  height: 60px;
                }
                .pulse-dot {
                  position: absolute;
                  top: 15px; left: 15px;
                  width: 30px; height: 30px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-dot 1.25s cubic-bezier(0.455, 0.03, 0.515, 0.955) -.4s infinite;
                  display: flex;
                  align-items: center;
                  justify-content: center;
                  z-index: 2;
                }
                .pulse-ring {
                  position: absolute;
                  top: 0; left: 0;
                  width: 60px; height: 60px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-ring 1.25s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
                  z-index: 1;
                }
                .thinking-text {
                  margin-top: 15px;
                  font-family: sans-serif;
                  color: #007bff;
                  font-weight: bold;
                  letter-spacing: 1px;
                }
                </style>
                
                <div class="ai-thinking-container">
                    <div class="pulse-wrapper">
                        <div class="pulse-ring"></div>
                        <div class="pulse-dot">
                            <span style="font-size: 18px;">ğŸ§ª</span>
                        </div>
                    </div>
                    <div class="thinking-text">åŠªåŠ›åˆ†æä¸­...</div>
                </div>
                """, 
                unsafe_allow_html=True
            )


        # æœ€ç»ˆå›ç­”æ‰“å­—æœºæ˜¾ç¤ºçš„å ä½ç¬¦
        response_placeholder = st.empty()
        
        if not graph_data and not vector_docs:
            thinking_container.empty() # æ²¡æ‰¾åˆ°æ•°æ®ï¼Œç›´æ¥æ¸…é™¤æç¤º
            response_placeholder.warning("âš ï¸ æ ¹æ®ç›®å‰çŸ¥è¯†åº“è®°å½•ï¼Œæœªæ‰¾åˆ°ä¸è¯¥æé—®ç›¸å…³çš„ä½å·äº‹å®æˆ–æ–‡æ¡£è¯´æ˜ã€‚")
        else:
             # === [ä¿®æ”¹å¼€å§‹] ä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•° ===
            
            # 1. å°†å›¾æ•°æ®è½¬æ¢ä¸ºé“¾å¼å™è¿°æ–‡æœ¬
            current_intent = intent_res.get('intent', 'Info_Query')
            graph_text_narrative = format_graph_data(graph_data, current_intent)
            
            # 2. æ„é€ æ›´æ¸…æ™°çš„ä¸Šä¸‹æ–‡
            h_context = f"""
ã€å›¾è°±äº‹å® (ç‰©ç†æ‹“æ‰‘ä¸å·¥è‰ºè¯­ä¹‰)ã€‘:
{graph_text_narrative}

ã€çŸ¥è¯†åº“æ–‡æ¡£ (æ“ä½œè§„ç¨‹ä¸åŸç†)ã€‘:
{' '.join(vector_docs)}
            """
            # === [ä¿®æ”¹ç»“æŸ] ===
            
            # --- æç¤ºè¯å¾®è°ƒ (ç¡®ä¿æ¨¡å‹ä¸ä¼šå¤ªå•°å—¦) ---
            intent_guidance = {
            "Path_Analysis": "å½“å‰ä»»åŠ¡æ˜¯ã€å·¥è‰ºæµç¨‹åˆ†æã€‘ï¼Œè¯·é‡ç‚¹æè¿°è®¾å¤‡çš„ä½œç”¨ã€ç‰©æ–™æµå‘ã€è…”å®¤åˆ‡æ¢åŠä»‹è´¨å˜åŒ–ã€‚",
            "Fault_Diagnosis": "å½“å‰ä»»åŠ¡æ˜¯ã€æ•…éšœè¯Šæ–­ã€‘ï¼Œè¯·åˆ†æä¸Šæ¸¸å¯èƒ½çš„é£é™©æºã€‚",
            "Status_Check": "å½“å‰ä»»åŠ¡æ˜¯ã€çŠ¶æ€æ£€æŸ¥ã€‘ï¼Œè¯·æ ¸å¯¹è®¾å¤‡å‚æ•°ä¸ä»ªè¡¨ç›‘æ§èŒƒå›´ã€‚",
            "Procedure_Query": "å½“å‰ä»»åŠ¡æ˜¯ã€è§„ç¨‹æŸ¥è¯¢ã€‘ï¼Œè¯·è¯¦ç»†è¯´æ˜æ“ä½œæ­¥éª¤å’Œå®‰å…¨è¦æ±‚ã€‚",
            "Info_Query": "å½“å‰ä»»åŠ¡æ˜¯ã€ä¿¡æ¯æŸ¥è¯¢ã€‘ï¼Œè¯·è§£é‡Šç›¸å…³ä½å·çš„åŠŸèƒ½å®šä¹‰ã€‚"
        }
        task_context = intent_guidance.get(current_intent, "")

            # ä¿®æ”¹åŸæœ‰çš„ sys_promptï¼Œåœ¨å¼€å¤´æ³¨å…¥ task_context
        sys_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„åŒ–å·¥è£…ç½®å·¥è‰ºå·¥ç¨‹å¸ˆã€‚
                
                ### 0. å½“å‰ä»»åŠ¡é‡ç‚¹
                {task_context}


                ### 1. çŸ¥è¯†å›´æ  (Knowledge Guardrails) - æ ¸å¿ƒå‡†åˆ™
                - **ä»…é™ä¸Šä¸‹æ–‡å›ç­”**ï¼šä½ åªèƒ½æ ¹æ®ä¸‹æ–¹æä¾›çš„ã€å›¾è°±äº‹å®ã€‘å’Œã€æ–‡æ¡£èµ„æ–™ã€‘è¿›è¡Œå›ç­”ã€‚ä¸¥ç¦ä½¿ç”¨ä½ è‡ªèº«è®­ç»ƒæ•°æ®ä¸­å…³äºç‰¹å®šå·¥å‚ã€ç‰¹å®šä½å·çš„å¤–éƒ¨çŸ¥è¯†ã€‚
                - **ä¸¥ç¦æ¨æµ‹è¿æ¥**ï¼šå¦‚æœã€å›¾è°±äº‹å®ã€‘ä¸­æ²¡æœ‰æ˜¾ç¤º A è®¾å¤‡ä¸ B è®¾å¤‡ä¹‹é—´çš„è·¯å¾„ï¼Œå³ä½¿åœ¨å¸¸è§„å·¥è‰ºä¸­å®ƒä»¬é€šå¸¸ç›¸è¿ï¼Œä½ ä¹Ÿå¿…é¡»å›ç­”â€œå½“å‰å›¾è°±æœªè®°å½• A ä¸ B çš„ç›´æ¥è¿æ¥â€ã€‚
                - **è¯šå®å‘ŠçŸ¥ç¼ºå¤±**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®çš„ä½å·åœ¨ã€å›¾è°±äº‹å®ã€‘ä¸­ä¸å­˜åœ¨ï¼Œæˆ–è€…è¯¢é—®çš„æ“ä½œåœ¨ã€æ–‡æ¡£èµ„æ–™ã€‘ä¸­æœªæåŠï¼Œè¯·æ˜ç¡®å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰çŸ¥è¯†åº“è®°å½•ï¼Œæ— æ³•æä¾›å…³äº [ä½å·/æ“ä½œ] çš„ä¿¡æ¯â€ã€‚
                - **ç¦æ­¢å¹»è§‰è¡¥å…¨**ï¼šä¸¥ç¦ä¸ºäº†ä½¿æµç¨‹å®Œæ•´è€Œè‡ªè¡Œè¡¥å…¨ä¸­é—´çš„é˜€é—¨ã€ç®¡æ®µæˆ–ä»ªè¡¨ã€‚

                ### 2. è¯æ®æº¯æºè¦æ±‚
                - ä½ çš„æ¯ä¸€å¥å…³é”®ç»“è®ºéƒ½åº”æš—ç¤ºå…¶æ¥æºã€‚
                - æ¶‰åŠç‰©ç†è¿æ¥ã€ä»‹è´¨ã€æè´¨ã€è…”å®¤é€»è¾‘æ—¶ï¼Œè¯·è¡¨è¿°ä¸ºï¼šâ€œæ ¹æ®å›¾è°±æ‹“æ‰‘è®°å½•...â€ã€‚
                - æ¶‰åŠæ“ä½œæ­¥éª¤ã€å®‰å…¨è¦æ±‚ã€å·¥è‰ºåŸç†æ—¶ï¼Œè¯·è¡¨è¿°ä¸ºï¼šâ€œæ ¹æ®æ“ä½œè§„ç¨‹è®°è½½...â€ã€‚

                ### 3. ç‰©ç†è¯­ä¹‰çº¦æŸ
                - å¿…é¡»å°Šé‡è…”å®¤é€»è¾‘ï¼šæ˜ç¡®åŒºåˆ†å£³ç¨‹(ShellSide)ã€ç®¡ç¨‹(TubeSide)ã€å¤¹å¥—(Jacket)ã€‚å¦‚æœç‰©æ–™æµå‘äº†é”™è¯¯çš„è…”å®¤ï¼Œè¯·åœ¨å›ç­”ä¸­ä½œä¸ºæ½œåœ¨é£é™©ç‚¹æŒ‡å‡ºã€‚

                ### 4. å›ç­”é£æ ¼
                - é£æ ¼ï¼šæå…¶ä¸“ä¸šã€å†·å³»ã€å®¢è§‚ã€‚
                - ç»“æ„ï¼š
                1. ã€æ ¸å¿ƒç»“è®ºã€‘ï¼šä¸€å¥è¯ç›´æ¥å›ç­”é—®é¢˜ï¼ŒåŒ…æ‹¬è®¾å¤‡çš„ä½å·å’Œåç§°ã€‚
                2.  æ ¹æ®ä»»åŠ¡é‡ç‚¹è¯¦ç»†è®²è§£ã€‚
                3. ã€å®‰å…¨æé†’ã€‘ï¼šï¼ˆå¦‚æœ‰ï¼‰åŸºäºäº‹å®çš„é£é™©å‘ŠçŸ¥ã€‚

                ### 5. è´Ÿé¢çº¦æŸ (Negative Constraints)
                - ç»å¯¹ç¦æ­¢ä½¿ç”¨ï¼š â€œæˆ‘çŒœâ€ã€â€œé€šå¸¸æƒ…å†µä¸‹â€ã€â€œç»éªŒè¡¨æ˜â€ã€â€œå¯èƒ½â€ã€‚
                - ç»å¯¹ç¦æ­¢å›ç­”ï¼š ä¸å½“å‰è£…ç½®æ— å…³çš„é€šç”¨åŒ–å·¥å¸¸è¯†ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¯¢é—®å®šä¹‰ï¼‰ã€‚
                """

        try:
                # è°ƒç”¨æ¨¡å‹
                stream = ollama.chat(model=LLM_MODEL, messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': f"äº‹å®èƒŒæ™¯: {h_context}\né—®é¢˜: {prompt}"}
                ], stream=True)

                # --- æ ¸å¿ƒæ”¹è¿›ï¼šç²¾ç¡®æ§åˆ¶æ¸…é™¤æ—¶æœº ---
                for chunk in stream:
                    content = chunk['message']['content']
                    
                    # åªæœ‰å½“æ¨¡å‹çœŸæ­£è¾“å‡ºäº†å†…å®¹ï¼ˆä¸”éç©ºï¼‰æ—¶ï¼Œæ‰æ¸…é™¤â€œæ€è€ƒä¸­â€æç¤º
                    if content.strip() and not full_response:
                        thinking_container.empty()
                    
                    full_response += content
                    # åŠ¨æ€å±•ç¤ºæ‰“å­—æœºæ•ˆæœ
                    response_placeholder.markdown(full_response + "â–Œ")
                
                # å®Œæˆè¾“å‡ºï¼Œç§»é™¤å…‰æ ‡
                response_placeholder.markdown(full_response)
                
        except Exception as e:
                thinking_container.empty()
                st.error(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")

        # 3. è¯æ®æº¯æºæ˜¾ç¤º (ä¼˜åŒ–ç‰ˆ)
        if graph_data or vector_docs:
            with st.expander("ğŸ” åŸå§‹æ£€ç´¢è¯æ®"):
                tab1, tab2 = st.tabs(["å›¾è°±äº‹å® (é“¾å¼å™è¿°)", "æ–‡æ¡£ç‰‡æ®µ"])
                
                with tab1:
                    # === [ä¿®æ”¹] ä½¿ç”¨ format_graph_data æ¸²æŸ“ ===
                    if graph_data:
                        # å¤ç”¨ä¹‹å‰è®¡ç®—å¥½çš„ intent
                        current_intent = intent_res.get('intent', 'Info_Query')
                        formatted_text = format_graph_data(graph_data, current_intent)
                        st.markdown(formatted_text)
                    else:
                        st.info("æ— å›¾è°±æ•°æ®")
                
                with tab2:
                    if vector_docs:
                        for i, d in enumerate(vector_docs):
                            st.info(f"**ç‰‡æ®µ {i+1}**:\n{d}")
                    else:
                        st.info("æ— æ–‡æ¡£æ•°æ®")

    # å°†åŠ©æ‰‹çš„å›ç­”å­˜å…¥å¯¹è¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})