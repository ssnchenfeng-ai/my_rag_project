import streamlit as st
import sys
from dotenv import load_dotenv
import os
import re
import json
import ollama
import chromadb
import frontmatter
from neo4j import GraphDatabase

# ================= 0. åŠ è½½ç¯å¢ƒå˜é‡ =================
load_dotenv()  # è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶

# ================= 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½® =================
# ä½¿ç”¨ os.getenv('å˜é‡å', 'é»˜è®¤å€¼') çš„æ–¹å¼è¯»å–
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")  # å¯†ç ä¸å»ºè®®è®¾é»˜è®¤å€¼

CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chemical_kb")

EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text:latest")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-r1:latest")

# è·å–æœ¬åœ°æ•°æ®è·¯å¾„
DEFAULT_DATA_PATH = os.getenv("DATA_PATH", "./data/")

# ================= 2. åˆå§‹åŒ–è¿æ¥ =================
neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
# --- åœ¨è¿™é‡Œæ’å…¥ã€é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°ã€‘ ---
def recursive_split_text(text, max_chars=1200, overlap=200):
    """å½“æ®µè½è¿‡é•¿æ—¶ï¼ŒæŒ‰ä¼˜å…ˆçº§è¿›è¡Œé€’å½’æ‹†åˆ†"""
    if len(text) <= max_chars:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunk = text[start:end]
        if end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œæˆ–å¥å·ï¼Œé¿å…ç”Ÿç¡¬åˆ‡æ–­
            last_break = max(chunk.rfind('\n'), chunk.rfind('ã€‚'), chunk.rfind('. '))
            if last_break > max_chars * 0.5: 
                end = start + last_break + 1
                chunk = text[start:end]
        chunks.append(chunk)
        start += (len(chunk) - overlap)
        if len(chunk) <= overlap: break
    return chunks

def extract_tags(text):
    # 1. åŒ¹é…æ¨¡å¼ï¼š
    # ([a-zA-Z]{1,3})  -> æ•è·1-3ä½å­—æ¯å‰ç¼€ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # [-_]?            -> åŒ¹é…å¯é€‰çš„è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿
    # (\d{2,4})        -> æ•è·2-4ä½æ•°å­—
    # ([a-zA-Z]?)      -> æ•è·å¯é€‰çš„ä¸€ä½å­—æ¯åç¼€
    pattern = r'([a-zA-Z]{1,3})[\s\-_]?(\d{2,4})([a-zA-Z]?)'
    # æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ç»„åˆ
    matches = re.findall(pattern, text)
    
    normalized_tags = []
    for prefix, digits, suffix in matches:
        # 2. æ ‡å‡†åŒ–å¤„ç†ï¼š
        # å…¨éƒ¨è½¬ä¸ºå¤§å†™ï¼Œå¹¶åœ¨å­—æ¯ä¸æ•°å­—ä¹‹é—´å¼ºåˆ¶åŠ ä¸Š "-"
        # ä¾‹å­ï¼šd43 -> D-43, D_43 -> D-43, d-43a -> D-43A
        standard_tag = f"{prefix.upper()}-{digits}{suffix.upper()}"
        normalized_tags.append(standard_tag)
    
    # è¿”å›å»é‡åçš„ç»“æœ
    return list(set(normalized_tags))

def clean_markdown(content):
    content = re.sub(r'^\\---', '---', content, flags=re.MULTILINE)
    content = re.sub(r'^\\(#+)', r'\1', content, flags=re.MULTILINE)
    content = re.sub(r'[\u200B-\u200D\uFEFF]', '', content)
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.match(r'^\|[\s*:|-]+\|$', line.strip()): continue 
        if line.strip().startswith('|'):
            line = line.strip('|').replace('|', '   ')
        cleaned_lines.append(line)
    return '\n'.join(cleaned_lines)

def hierarchical_chunking(content, file_path):
    file_name = os.path.basename(file_path).replace('.md', '')
    post = frontmatter.loads(content)
    doc_metadata = post.metadata
    main_content = post.content
    doc_title = doc_metadata.get('title', file_name)
    final_chunks = []
    
    # è®¾å®šå‚æ•°
    MAX_CHUNK_LEN = 1200 
    OVERLAP_LEN = 200

    h3_blocks = re.split(r'(?=^###\s+)', main_content, flags=re.MULTILINE)
    for i, h3_block in enumerate(h3_blocks):
        h3_block = h3_block.strip()
        if not h3_block: continue
        
        h3_match = re.search(r'^###\s+(.*)$', h3_block, flags=re.MULTILINE)
        h3_title = h3_match.group(1).strip() if h3_match else "æ¦‚è§ˆ"
        h3_content = re.sub(r'^###\s+.*$', '', h3_block, flags=re.MULTILINE).strip()
        
        if '#### ' in h3_content:
            h4_blocks = re.split(r'(?=^####\s+)', h3_content, flags=re.MULTILINE)
            for j, h4_block in enumerate(h4_blocks):
                h4_block = h4_block.strip()
                if len(h4_block) < 20: continue
                
                h4_match = re.search(r'^####\s+(.*)$', h4_block, flags=re.MULTILINE)
                h4_title = h4_match.group(1).strip() if h4_match else ""
                content_body = re.sub(r'^####\s+.*$', '', h4_block, flags=re.MULTILINE).strip()
                
                breadcrumb = f"{doc_title} > {h3_title}" + (f" > {h4_title}" if h4_title else "")
                
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(content_body, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-{j}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
        else:
            if len(h3_content) > 20:
                breadcrumb = f"{doc_title} > {h3_title}"
                # --- è°ƒç”¨é€’å½’æ‹†åˆ† ---
                sub_parts = recursive_split_text(h3_content, MAX_CHUNK_LEN, OVERLAP_LEN)
                for k, part in enumerate(sub_parts):
                    final_chunks.append({
                        "id": f"{file_name}-{i}-p{k}",
                        "text": f"ã€è¯­å¢ƒï¼š{breadcrumb}ã€‘\n{part}",
                        "metadata": {**doc_metadata, "breadcrumb": breadcrumb, "source": file_path}
                    })
    return final_chunks

def analyze_intent_with_llm(prompt, extracted_tags):
    # å°†æ¨¡å‹æç¤ºè¯ä¹Ÿæ”¹ä¸ºä¸­æ–‡ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å‡†ç¡®åœ°æŒ‰ä¸­æ–‡é€»è¾‘æ€è€ƒ
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šæ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜çš„æ„å›¾å¹¶è¿”å› JSONã€‚
    å¯é€‰æ„å›¾ï¼š
    - Path_Analysis: è¯¢é—®ç‰©æ–™æµå‘ã€è·¯å¾„ã€ç»è¿‡å“ªé‡Œã€‚
    - Fault_Diagnosis: è¯¢é—®æ•…éšœåŸå› ã€ä¸Šæ¸¸æº¯æºã€‚
    - Status_Check: è¯¢é—®è®¾å¤‡è®¾è®¡å‚æ•°ã€ç›‘æ§ä»ªè¡¨ã€‚
    - Procedure_Query: è¯¢é—®æ“ä½œæ­¥éª¤ã€ç†ŸåŒ–æµç¨‹ã€‚
    - Info_Query: è¯¢é—®åŸºæœ¬å®šä¹‰æˆ–é€šç”¨ä¿¡æ¯ã€‚

    ç”¨æˆ·å·²æå–ä½å·ï¼š{extracted_tags}
    è¿”å›æ ¼å¼ï¼š{{"intent": "æ„å›¾åç§°", "start_node": "èµ·ç‚¹ä½å·", "end_node": "ç»ˆç‚¹ä½å·", "target_name": "è®¾å¤‡åç§°"}}"""
    try:
        # è¿™é‡Œå»ºè®®ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªèƒ½èŠå¤©çš„æ¨¡å‹
        response = ollama.chat(model=LLM_MODEL, messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': prompt}
        ], format='json')
        return json.loads(response['message']['content'])
    except:
        return {"intent": "Info_Query"}

def build_cypher(llm_result, extracted_tags, user_text):
    intent = llm_result.get("intent", "Info_Query")
    tags = extracted_tags
    
    # å¼ºåˆ¶ä¿®æ­£ï¼šå¦‚æœæ¶‰åŠä¸¤ä¸ªä½å·ä¸”åŒ…å«æµç¨‹åŠ¨è¯ï¼Œè®¾ä¸ºè·¯å¾„åˆ†æ
    if len(tags) >= 2 and any(k in user_text for k in ["åˆ°", "æµ", "ç»è¿‡", "å»å¾€", "æµç¨‹", "è”ç³»", "å·¥è‰º"]):
        intent = "Path_Analysis"
        
    cypher = ""; params = {}

    if intent == "Path_Analysis":
        start = llm_result.get("start_node") or (tags[0] if tags else None)
        end = llm_result.get("end_node") or (tags[1] if len(tags)>1 else None)
        
        if start and end:
            # è·¯å¾„åˆ†æï¼šè¿”å›æå…¶è¯¦å°½çš„èŠ‚ç‚¹å’Œç®¡é“å±æ€§
            cypher = """
            MATCH (start:Asset), (end:Asset)
    WHERE (start.Tag STARTS WITH $startTag OR replace(start.Tag, '-', '') = $startTagAlt)
      AND (end.Tag STARTS WITH $endTag OR replace(end.Tag, '-', '') = $endTagAlt)
    
    // 1. å¯»æ‰¾é¡ºæµæ–¹å‘çš„æœ€çŸ­è·¯å¾„
    MATCH path = shortestPath((start)-[:PIPE|MEASURES*..30]->(end))
    
    // 2. æ ¼å¼åŒ–è¿”å›ï¼šä¿ç•™æ‰€æœ‰ç‰©ç†è¯­ä¹‰å¹¶æŒ‰å·¥è‰ºé¡ºåºäº¤ç»‡
    RETURN 
        'Path_Analysis' as intent,
        [i IN range(0, length(path)-1) | {
            // èµ·ç‚¹è®¾å¤‡
            from_equipment: CASE 
                WHEN nodes(path)[i].Tag <> "TEE" AND nodes(path)[i].type <> "Instrument" AND nodes(path)[i].type <> "TappingPoint"
                THEN {tag: nodes(path)[i].Tag, desc: nodes(path)[i].desc, type: nodes(path)[i].type}
                ELSE "è¾…åŠ©è¿æ¥ç‚¹(TEE/æµ‹ç‚¹)" 
            END,
            
            // ç®¡é“è¯­ä¹‰ï¼ˆ12é¡¹å®Œæ•´å±æ€§ï¼‰
            pipeline_semantics: {
                fluid: relationships(path)[i].fluid,
                dn: relationships(path)[i].dn,
                material: relationships(path)[i].material,
                insulation: relationships(path)[i].insulation,
                pn: relationships(path)[i].pn,
                fromPort: relationships(path)[i].fromPort,
                toPort: relationships(path)[i].toPort,
                fromDesc: relationships(path)[i].fromDesc,
                toDesc: relationships(path)[i].toDesc,
                fromRegion: relationships(path)[i].fromRegion,
                toRegion: relationships(path)[i].toRegion,
                tag: relationships(path)[i].tag
            },
            
            // ç»ˆç‚¹è®¾å¤‡
            to_equipment: CASE 
                WHEN nodes(path)[i+1].Tag <> "TEE" AND nodes(path)[i+1].type <> "Instrument" AND nodes(path)[i+1].type <> "TappingPoint"
                THEN {tag: nodes(path)[i+1].Tag, desc: nodes(path)[i+1].desc, type: nodes(path)[i+1].type}
                ELSE "è¾…åŠ©è¿æ¥ç‚¹(TEE/æµ‹ç‚¹)" 
            END
        }] as structured_process_flow,
        length(path) as total_hops
            """
            params = {
                "startTag": start, "startTagAlt": start.replace("-", ""),
                "endTag": end, "endTagAlt": end.replace("-", "")
            }

    elif intent == "Fault_Diagnosis":
        # æ•…éšœè¯Šæ–­ï¼šä¾§é‡äºè¿½æº¯ä¸Šæ¸¸è®¾å¤‡åŠå…¶æè¿°
        cypher = """
        UNWIND $tags AS qTag
        
        // 1. æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç›®æ ‡è®¾å¤‡
        MATCH (target:Asset) 
        WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        
        // 2. æŸ¥æ‰¾ä¸Šæ¸¸è·¯å¾„ (ä½¿ç”¨ path å˜é‡æ•è·å®Œæ•´æ‹“æ‰‘)
        // è¿™é‡ŒæŸ¥æ‰¾ 1 åˆ° 3 è·³çš„ä¸Šæ¸¸è®¾å¤‡ï¼Œæ’é™¤ TEE (ä¸‰é€š) è¿™ç§æ— æ„ä¹‰èŠ‚ç‚¹ä½œä¸ºç»ˆç‚¹ï¼Œä½†ä¿ç•™è·¯å¾„ä¸­çš„å…³ç³»
        MATCH path = (target)<-[:PIPE*1..3]-(source:Asset)
        WHERE source.Tag <> 'TEE'
        
        // 3. å±•å¼€è·¯å¾„ä¸­çš„æ¯ä¸€æ®µå…³ç³» (Relationship)
        UNWIND relationships(path) AS r
        
        // 4. æå–å…³ç³»çš„èµ·ç‚¹(start)å’Œç»ˆç‚¹(end)
        // æ³¨æ„ï¼šè™½ç„¶æˆ‘ä»¬æ˜¯å¾€ä¸Šæ¸¸æŸ¥ï¼Œä½†ç‰©ç†æµå‘ä¾ç„¶æ˜¯ start -> end
        WITH target, startNode(r) AS start, endNode(r) AS end, r
        
        // 5. è¿”å›ç»“æ„åŒ–çš„æ‹“æ‰‘æ•°æ®
        RETURN target.Tag as tag, 
               'Fault_Diagnosis' as intent, 
               collect(DISTINCT {
                   // è¿çº¿èµ·ç‚¹ (ä¸Šæ¸¸)
                   source: start.Tag,
                   source_type: start.type,
                   source_desc: start.desc,
                   
                   // è¿çº¿ç»ˆç‚¹ (ä¸‹æ¸¸)
                   target: end.Tag,
                   target_type: end.type,
                   
                   // === æ ¸å¿ƒç‰©ç†è¯­ä¹‰ (AI è¯Šæ–­çš„å…³é”®) ===
                   fluid: r.fluid,           // ä»‹è´¨ (å¦‚: Steam, Water)
                   from_region: r.fromRegion,// èµ·ç‚¹åŒºåŸŸ (å¦‚: ShellSide)
                   to_region: r.toRegion,    // ç»ˆç‚¹åŒºåŸŸ (å¦‚: TubeSide) -> è¯Šæ–­ä¸²æ–™/å¹²çƒ§çš„å…³é”®
                   insulation: r.insulation  // ä¿æ¸©/ä¼´çƒ­ -> è¯Šæ–­å†»ç»“/ç»“æ™¶çš„å…³é”®
               }) as upstream_trace
        """
        params = {"tags": tags}

    elif intent == "Status_Check":
        # ä»ªè¡¨æ£€æŸ¥ï¼šä¾§é‡äº MEASURES å…³ç³»å’Œ Instrument èŠ‚ç‚¹çš„å‚æ•°
        cypher = """
        UNWIND $tags AS qTag
        MATCH (target:Asset) WHERE target.Tag = qTag OR replace(target.Tag, '-', '') = replace(qTag, '-', '')
        OPTIONAL MATCH (target)-[:MEASURES]-(sensor:Instrument)
        RETURN target.Tag as tag, target.desc as desc, 
               {temp: target.design_temp, press: target.design_press, spec: target.spec} as design_params,
               collect(DISTINCT {tag: sensor.Tag, desc: sensor.desc, range: sensor.range, unit: sensor.unit}) as sensors
        """
        params = {"tags": tags}

    else:
        # åŸºç¡€æŸ¥è¯¢ï¼šè¿”å›ä½å·ã€æè¿°å’Œç±»å‹
        cypher = """
         UNWIND $tags AS qTag
        
        // 1. æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ä¸­å¿ƒè®¾å¤‡
        MATCH (center:Asset) 
        WHERE center.Tag = qTag OR replace(center.Tag, '-', '') = replace(qTag, '-', '')
        
        // 2. åŒå‘æ‰©å±•ï¼šæŸ¥æ‰¾è·ç¦»ä¸­å¿ƒè®¾å¤‡ 1 åˆ° 3 è·³çš„æ‰€æœ‰è·¯å¾„
        // æ³¨æ„è¿™é‡Œæ²¡æœ‰ç®­å¤´ï¼Œè¡¨ç¤ºåŒå‘æŸ¥æ‰¾ (Upstream & Downstream)
        // åŒ…å« PIPE (ç®¡çº¿), CONTROLS (æ§åˆ¶), MEASURES (æµ‹é‡)
        MATCH path = (center)-[:PIPE|CONTROLS|MEASURES*1..3]-(neighbor:Asset)
        
        // 3. å±•å¼€è·¯å¾„ä¸­çš„æ¯ä¸€æ®µå…³ç³»
        UNWIND relationships(path) AS r
        
        // 4. æå–ç‰©ç†æµå‘ (æ— è®ºæŸ¥è¯¢æ–¹å‘å¦‚ä½•ï¼ŒstartNode->endNode æ°¸è¿œä»£è¡¨ç‰©ç†æµå‘)
        WITH center, startNode(r) AS source, endNode(r) AS target, r, type(r) as relType
        
        // 5. è¿‡æ»¤æ‰æ— æ„ä¹‰çš„çº¯è¿æ¥èŠ‚ç‚¹ (å¦‚ TEE)ï¼Œé™¤éå®ƒæ˜¯è·¯å¾„çš„ä¸­é—´ç¯èŠ‚
        // (è¿™é‡Œé€‰æ‹©ä¿ç•™ TEE çš„è¿æ¥å…³ç³»ï¼Œä½†åœ¨å±•ç¤ºæ—¶ç”±å‰ç«¯æˆ– LLM å†³å®šæ˜¯å¦å¿½ç•¥)
        
        // 6. è¿”å›å»é‡åçš„æ‹“æ‰‘ç»“æ„
        RETURN center.Tag as tag, 
               'Info_Query' as intent,
               // æ±‡æ€»è¯¥è®¾å¤‡å‘¨å›´çš„æ‰€æœ‰å±æ€§
               {
                   type: center.type,
                   desc: center.desc,
                   spec: center.spec,
                   material: center.material
               } as self_info,
               collect(DISTINCT {
                   // å…³ç³»ç±»å‹ (PIPE/CONTROLS/MEASURES)
                   type: relType,
                   
                   // èµ·ç‚¹ (æµå‡ºæ–¹)
                   source: source.Tag,
                   source_type: source.type,
                   
                   // ç»ˆç‚¹ (æµå…¥æ–¹)
                   target: target.Tag,
                   target_type: target.type,
                   
                   // ç‰©ç†è¯­ä¹‰ç»†èŠ‚
                   fluid: r.fluid,
                   from_region: r.fromRegion, // å…³é”®ï¼šä»å“ªä¸ªè…”å®¤å‡ºæ¥
                   to_region: r.toRegion,     // å…³é”®ï¼šè¿›å“ªä¸ªè…”å®¤
                   tag: r.tag                 // ç®¡æ®µå·
               }) as topology
        """
        params = {"tags": tags}

    return cypher, params

def query_neo4j(query, params):
    if not query: return []
    # ç»ˆç«¯è°ƒè¯•ä¿¡æ¯ä¿ç•™è‹±æ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    print(f"\n[è°ƒè¯•] æ‰§è¡Œ Cypher: {query}\n[è°ƒè¯•] å‚æ•°: {params}", file=sys.stderr, flush=True)
    try:
        with neo4j_driver.session() as session:
            result = session.run(query, **params)
            return [dict(record) for record in result]
    except Exception as e:
        print(f"[é”™è¯¯] Neo4j æŸ¥è¯¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
        return []
# ==============================================================================
# ğŸ‘‡ğŸ‘‡ğŸ‘‡ è¯·åœ¨è¿™é‡Œæ’å…¥æ–°å¢çš„è¾…åŠ©å‡½æ•° ğŸ‘‡ğŸ‘‡ğŸ‘‡
# ==============================================================================

def translate_region(region_code):
    """å°†è‹±æ–‡åŒºåŸŸä»£ç ç¿»è¯‘ä¸ºä¸­æ–‡è¯­ä¹‰"""
    if not region_code: return "é€šç”¨æ¥å£"
    mapping = {
        'ShellSide': 'å£³ç¨‹',
        'ShellSide:Vapor': 'å£³ç¨‹(æ°”ç›¸)',
        'ShellSide:Liquid': 'å£³ç¨‹(æ¶²ç›¸)',
        'TubeSide': 'ç®¡ç¨‹',
        'TubeSide:Liquid': 'ç®¡ç¨‹(æ¶²ç›¸)',
        'TubeSide:Vapor': 'ç®¡ç¨‹(æ°”ç›¸)',
        'Jacket': 'å¤¹å¥—',
        'InnerVessel': 'å†…èƒ†',
        'ControlSignal': 'æ§åˆ¶ä¿¡å·æ¥å£',
        'UpperSaltChannel': 'ä¸Šç›é“',
        'LowerSaltChannel': 'ä¸‹ç›é“'
    }
    return mapping.get(region_code, region_code)

def format_graph_data(data, intent):
    """
    å°† Neo4j è¿”å›çš„ JSON åˆ—è¡¨è½¬æ¢ä¸º LLM å‹å¥½çš„é“¾å¼å™è¿°æ–‡æœ¬
    å¢å¼ºç‰ˆï¼šæ˜ç¡®æ ‡æ³¨äº†æ¥æºç«¯å£(fromRegion)å’Œåˆ°è¾¾ç«¯å£(toRegion)
    """
    if not data:
        return "æœªæŸ¥è¯¢åˆ°ç›¸å…³å›¾è°±æ•°æ®ã€‚"
    
    text_lines = []
    
    # === åœºæ™¯ 1: è·¯å¾„åˆ†æ (Path_Analysis) ===
    if intent == "Path_Analysis":
        for path_idx, record in enumerate(data):
            text_lines.append(f"ğŸ›£ï¸ **ç‰©ç†è·¯å¾„ #{path_idx + 1} (æ€»è·³æ•°: {record.get('total_hops', 0)})**:")
            steps = record.get('structured_process_flow', [])
            
            for i, step in enumerate(steps):
                # 1. æå–èµ·ç‚¹åŠæ¥æºç«¯å£
                src = step['from_equipment']
                pipe = step['pipeline_semantics']
                
                src_tag = src['tag'] if isinstance(src, dict) else src
                src_desc = f"({src['desc']})" if isinstance(src, dict) and src.get('desc') else ""
                from_reg = translate_region(pipe.get('fromRegion')) # æ–°å¢ï¼šæ¥æºç«¯å£
                
                # æ ¼å¼åŒ–èµ·ç‚¹ï¼šğŸ­ è®¾å¤‡ (æè¿°) [å‡ºå£: å£³ç¨‹]
                src_str = f"ğŸ­ **{src_tag}**{src_desc}"
                if from_reg != "é€šç”¨æ¥å£":
                    src_str += f" `[å‡ºå£: {from_reg}]`"
                
                # 2. ç®¡é“/å…³ç³»è¯­ä¹‰
                fluid = pipe.get('fluid', 'æœªçŸ¥ä»‹è´¨')
                p_tag = pipe.get('tag') or 'æ— ç®¡å·'
                insulation = pipe.get('insulation', 'None')
                conn_desc = f" ==( ğŸŒŠ{fluid} | ğŸ·ï¸{p_tag}"
                if insulation != 'None': conn_desc += f" | ğŸ”¥{insulation}"
                conn_desc += " )==> "
                
                # 3. æå–ç»ˆç‚¹åŠè¿›å…¥ç«¯å£
                tgt = step['to_equipment']
                tgt_tag = tgt['tag'] if isinstance(tgt, dict) else tgt
                tgt_desc = f"({tgt['desc']})" if isinstance(tgt, dict) and tgt.get('desc') else ""
                to_reg = translate_region(pipe.get('toRegion')) # ä¿æŒï¼šè¿›å…¥ç«¯å£
                
                # æ ¼å¼åŒ–ç»ˆç‚¹ï¼š[å…¥å£: ç®¡ç¨‹] ğŸ­ è®¾å¤‡ (æè¿°)
                tgt_str = f"**{tgt_tag}**{tgt_desc}"
                if to_reg != "é€šç”¨æ¥å£":
                    tgt_str = f"`[å…¥å£: {to_reg}]` ğŸ­ {tgt_str}"
                else:
                    tgt_str = f"ğŸ­ {tgt_str}"
                
                text_lines.append(f"   {i+1}. {src_str}{conn_desc}{tgt_str}")
            text_lines.append("") 

    # === åœºæ™¯ 2: æ•…éšœè¯Šæ–­ (Fault_Diagnosis) ===
    elif intent == "Fault_Diagnosis":
        for record in data:
            target_tag = record.get('tag')
            text_lines.append(f"ğŸ› ï¸ **ç›®æ ‡è®¾å¤‡**: {target_tag}")
            text_lines.append("   **ä¸Šæ¸¸æº¯æº (Upstream Trace):**")
            
            traces = record.get('upstream_trace', [])
            for trace in traces:
                source_tag = trace.get('source')
                from_reg = translate_region(trace.get('from_region')) # æ–°å¢ï¼šæ¥æºç«¯å£
                to_reg = translate_region(trace.get('to_region'))     # ä¿æŒï¼šè¿›å…¥ç«¯å£
                fluid = trace.get('fluid', 'Unknown')
                
                # å¢å¼ºç‰ˆè¯Šæ–­è¯­ä¹‰ï¼š[æ¥æºè®¾å¤‡][å‡ºå£æ¥å£] --(ä»‹è´¨)--> [ç›®æ ‡è®¾å¤‡][å…¥å£æ¥å£]
                line = f"   â¬†ï¸ æ¥æº: **{source_tag}** `[{from_reg}]` "
                line += f" --è¾“é€: {fluid}--> "
                line += f"è¿›å…¥ç›®æ ‡è®¾å¤‡çš„ **[{to_reg}]**"
                text_lines.append(line)
            text_lines.append("")

    # === åœºæ™¯ 3: ä¿¡æ¯æŸ¥è¯¢ (Info_Query) ===
    elif intent == "Info_Query":
        for record in data:
            self_info = record.get('self_info', {})
            text_lines.append(f"â„¹ï¸ **è®¾å¤‡æ¡£æ¡ˆ**: {record.get('tag')}")
            text_lines.append(f"   **è¯¦ç»†æ‹“æ‰‘ (Topology Detail):**")
            
            topo = record.get('topology', [])
            for t in topo:
                # è¯†åˆ«å½“å‰è®¾å¤‡æ˜¯èµ·ç‚¹è¿˜æ˜¯ç»ˆç‚¹
                is_source = (t.get('source') == record.get('tag'))
                neighbor = t.get('target') if is_source else t.get('source')
                direction = "â¡ï¸ æµå‡ºè‡³" if is_source else "â¬…ï¸ æ¥æ”¶æ¥è‡ª"
                
                # å…³é”®ï¼šåŒæ—¶å±•ç¤ºæœ¬ç«¯æ¥å£å’Œå¯¹ç«¯æ¥å£
                local_reg = translate_region(t.get('from_region') if is_source else t.get('to_region'))
                fluid = t.get('fluid', 'N/A')
                
                line = f"   - {direction} **{neighbor}** (ä»‹è´¨: {fluid} | æœ¬ç«¯æ¥å£: {local_reg})"
                text_lines.append(line)
            text_lines.append("")

    else:
        text_lines.append(json.dumps(data, ensure_ascii=False, indent=2))

    return "\n".join(text_lines)

# ==============================================================================
# ğŸ‘†ğŸ‘†ğŸ‘† æ’å…¥ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
# ============================================================================

# ================= 4. Streamlit ç•Œé¢æ˜¾ç¤º (ä¸­æ–‡ä¸­æ–‡åŒ–) =================
st.set_page_config(page_title="åŒ–å·¥çŸ¥è¯†å›¾è°±", layout="wide", page_icon="ğŸ§ª")
st.title("ğŸ§ª åŒ–å·¥è£…ç½®å›¾è°± + æ–‡æ¡£å‘é‡æ··åˆçŸ¥è¯†åº“")

# --- ä¾§è¾¹æ ï¼šç®¡ç†é¢æ¿ ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç³»ç»Ÿåå°ç®¡ç†")
    
    # æ˜¾ç¤ºå‘é‡åº“ç»Ÿè®¡
    try:
        db_count = collection.count()
        st.metric("å·²å­˜å‚¨çŸ¥è¯†åˆ‡ç‰‡æ•°", db_count)
    except:
        st.error("å‘é‡æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    st.markdown("---")
    
    # çŸ¥è¯†åŒæ­¥åŠŸèƒ½
    st.subheader("ğŸ“ æ•°æ®åŒæ­¥")
    path_input = st.text_area("Markdown æ–‡æ¡£è·¯å¾„", "/Users/chenfeng/åŸ¹è®­èµ„æ–™/åŒ–å·¥è‹¯é…åŸºæœ¬çŸ¥è¯†/")
    
    if st.button("ğŸš€ å¼€å§‹å¢é‡åŒæ­¥"):
        all_chunks = []
        with st.spinner("æ­£åœ¨æ‰«æå¹¶è§£ææ–‡æ¡£..."):
            if os.path.exists(path_input):
                for root, _, files in os.walk(path_input):
                    for file in files:
                        if file.endswith('.md'):
                            try:
                                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                                    cleaned = clean_markdown(f.read())
                                    chunks = hierarchical_chunking(cleaned, os.path.join(root, file))
                                    all_chunks.extend(chunks)
                            except: pass
            
        if all_chunks:
            total = len(all_chunks)
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, chunk in enumerate(all_chunks):
                status_text.text(f"æ­£åœ¨å‘é‡åŒ– ({idx+1}/{total}): {chunk['id']}")
                try:
                    emb = ollama.embeddings(model=EMBED_MODEL, prompt=chunk['text'][:2000])['embedding']
                    collection.upsert(
                        ids=[chunk['id']], 
                        embeddings=[emb], 
                        documents=[chunk['text']], 
                        metadatas=[{k:str(v) for k,v in chunk['metadata'].items()}]
                    )
                except: pass
                progress_bar.progress((idx + 1) / total)
            
            status_text.text("âœ… åŒæ­¥åœ†æ»¡å®Œæˆï¼")
            st.balloons()
            st.rerun()
        else:
            st.warning("æœªåœ¨è¯¥è·¯å¾„ä¸‹æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")

    if st.button("ğŸ—‘ï¸ å±é™©æ“ä½œï¼šæ¸…ç©ºå‘é‡åº“"):
        chroma_client.delete_collection(COLLECTION_NAME)
        collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
        st.warning("åº“å·²æ¸…ç©º")
        st.rerun()

# --- å¯¹è¯åŒºåŸŸ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ç”¨æˆ·è¾“å…¥
# --- å¯¹è¯åŒºåŸŸ (ä¼˜åŒ–ç‰ˆ) ---
if prompt := st.chat_input("æ‚¨å¯ä»¥é—®æˆ‘ï¼šD-14 ååº”å™¨çš„è®¾è®¡å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 1. æ£€ç´¢é˜¶æ®µ (ChromaDB + Neo4j)
        graph_data = []
        vector_docs = []
        
        with st.status("ğŸ” æ­£åœ¨æ£€ç´¢åŒåº“äº‹å®...", expanded=True) as status:
            extracted_tags = extract_tags(prompt)
            st.write(f"ğŸ·ï¸ **è¯†åˆ«ä½å·**: `{', '.join(extracted_tags) if extracted_tags else 'æœªè¯†åˆ«'}`")
            
            intent_res = analyze_intent_with_llm(prompt, extracted_tags)
            st.write(f"ğŸ¯ **è§£ææ„å›¾**: `{intent_res.get('intent', 'Info_Query')}`")
            
            cypher, params = build_cypher(intent_res, extracted_tags, prompt)
            if cypher:
                graph_data = query_neo4j(cypher, params)
                # --- è¿™é‡Œçš„é€»è¾‘æ”¹ä¸ºæ¡ä»¶æ˜¾ç¤º ---
                if graph_data:
                        st.write("âœ… **å›¾è°±äº‹å®**: å·²æˆåŠŸæ£€ç´¢åˆ°å…³è”æ‹“æ‰‘")
                else:
                     st.write("âš ï¸ **å›¾è°±äº‹å®**: æœªèƒ½åœ¨å›¾æ•°æ®åº“ä¸­æ‰¾åˆ°åŒ¹é…çš„è·¯å¾„æˆ–èŠ‚ç‚¹")
            
            q_emb = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)['embedding']
            vector_res = collection.query(query_embeddings=[q_emb], n_results=3)
            vector_docs = vector_res['documents'][0]
            st.write(f"ğŸ“„ **æ–‡æ¡£çŸ¥è¯†**: å·²åŒ¹é…ç›¸å…³æè¿°ç‰‡æ®µ")
            
            status.update(label=f"âœ… æ£€ç´¢å®Œæˆ: å‘½ä¸­ {len(graph_data)} æ¡äº‹å®, {len(vector_docs)} æ®µæ–‡æ¡£", state="complete", expanded=False)

        # 2. å›ç­”ç”Ÿæˆé˜¶æ®µ
        full_response = ""
        
        # --- ã€åŠ¨æ€å›¾æ ‡æ–¹æ¡ˆã€‘åˆ›å»ºä¸€ä¸ªåŒ–å­¦/AI åŠ¨æ€å ä½ç¬¦ ---
        thinking_container = st.empty()
        
        with thinking_container.container():
            st.markdown(
                """
                <style>
                @keyframes pulse-ring {
                  0% { transform: scale(.33); }
                  80%, 100% { opacity: 0; }
                }
                @keyframes pulse-dot {
                  0% { transform: scale(.8); }
                  50% { transform: scale(1); }
                  100% { transform: scale(.8); }
                }
                .ai-thinking-container {
                  display: flex;
                  flex-direction: column;
                  align-items: center;
                  justify-content: center;
                  padding: 20px;
                }
                .pulse-wrapper {
                  position: relative;
                  width: 60px;
                  height: 60px;
                }
                .pulse-dot {
                  position: absolute;
                  top: 15px; left: 15px;
                  width: 30px; height: 30px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-dot 1.25s cubic-bezier(0.455, 0.03, 0.515, 0.955) -.4s infinite;
                  display: flex;
                  align-items: center;
                  justify-content: center;
                  z-index: 2;
                }
                .pulse-ring {
                  position: absolute;
                  top: 0; left: 0;
                  width: 60px; height: 60px;
                  background-color: #007bff;
                  border-radius: 50%;
                  animation: pulse-ring 1.25s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
                  z-index: 1;
                }
                .thinking-text {
                  margin-top: 15px;
                  font-family: sans-serif;
                  color: #007bff;
                  font-weight: bold;
                  letter-spacing: 1px;
                }
                </style>
                
                <div class="ai-thinking-container">
                    <div class="pulse-wrapper">
                        <div class="pulse-ring"></div>
                        <div class="pulse-dot">
                            <span style="font-size: 18px;">ğŸ§ª</span>
                        </div>
                    </div>
                    <div class="thinking-text">åŠªåŠ›åˆ†æä¸­...</div>
                </div>
                """, 
                unsafe_allow_html=True
            )


        # æœ€ç»ˆå›ç­”æ‰“å­—æœºæ˜¾ç¤ºçš„å ä½ç¬¦
        response_placeholder = st.empty()
        
        if not graph_data and not vector_docs:
            thinking_container.empty() # æ²¡æ‰¾åˆ°æ•°æ®ï¼Œç›´æ¥æ¸…é™¤æç¤º
            response_placeholder.warning("âš ï¸ æ ¹æ®ç›®å‰çŸ¥è¯†åº“è®°å½•ï¼Œæœªæ‰¾åˆ°ä¸è¯¥æé—®ç›¸å…³çš„ä½å·äº‹å®æˆ–æ–‡æ¡£è¯´æ˜ã€‚")
        else:
             # === [ä¿®æ”¹å¼€å§‹] ä½¿ç”¨æ–°çš„æ ¼å¼åŒ–å‡½æ•° ===
            
            # 1. å°†å›¾æ•°æ®è½¬æ¢ä¸ºé“¾å¼å™è¿°æ–‡æœ¬
            current_intent = intent_res.get('intent', 'Info_Query')
            graph_text_narrative = format_graph_data(graph_data, current_intent)
            
            # 2. æ„é€ æ›´æ¸…æ™°çš„ä¸Šä¸‹æ–‡
            h_context = f"""
ã€å›¾è°±äº‹å® (ç‰©ç†æ‹“æ‰‘ä¸å·¥è‰ºè¯­ä¹‰)ã€‘:
{graph_text_narrative}

ã€çŸ¥è¯†åº“æ–‡æ¡£ (æ“ä½œè§„ç¨‹ä¸åŸç†)ã€‘:
{' '.join(vector_docs)}
            """
            # === [ä¿®æ”¹ç»“æŸ] ===
            
            # --- æç¤ºè¯å¾®è°ƒ (ç¡®ä¿æ¨¡å‹ä¸ä¼šå¤ªå•°å—¦) ---
            sys_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ–å·¥è£…ç½®ä¸“å®¶ã€‚è¯·ç»“åˆã€å›¾è°±äº‹å®ã€‘å’Œã€æ–‡æ¡£èµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚å¦‚æœã€å›¾è°±äº‹å®ã€‘å’Œã€çŸ¥è¯†åº“æ–‡æ¡£ã€‘ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå°±ç›´æ¥è¯´'æ ¹æ®æˆ‘ç°æœ‰çš„çŸ¥è¯†ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜'ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
            
             
            ### å›ç­”ç­–ç•¥
                        1. **ç»¼åˆåˆ¤æ–­**: å›¾è°±æä¾›äº†å‡†ç¡®çš„è®¾å¤‡ä½å·åŠŸèƒ½æè¿°å’Œè¿æ¥å…³ç³»çš„æ¥æºå»å‘ç­‰ï¼ŒçŸ¥è¯†åº“æä¾›äº†è¯¦ç»†çš„æ“ä½œæ­¥éª¤å’ŒåŸç†ã€‚
                        2 . **æ•…éšœè¯Šæ–­**: å¦‚æœå›¾è°±æ˜¾ç¤ºå¤šæ¡ä¾›æ–™æ”¯è·¯ï¼Œè¯·åˆ†åˆ«åˆ†æã€‚ç»“åˆçŸ¥è¯†åº“ä¸­çš„æ•…éšœå¤„ç†æ–¹æ³•ã€‚
                        3. **å†²çªå¤„ç†**: æ¶‰åŠè®¾å¤‡è¿æ¥å…³ç³»æ—¶ï¼Œä»¥å›¾è°±ä¸ºå‡†ï¼›æ¶‰åŠæ“ä½œç»†èŠ‚æ—¶ï¼Œä»¥çŸ¥è¯†åº“ä¸ºå‡†ã€‚

            
            """

            try:
                # è°ƒç”¨æ¨¡å‹
                stream = ollama.chat(model=LLM_MODEL, messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': f"äº‹å®èƒŒæ™¯: {h_context}\né—®é¢˜: {prompt}"}
                ], stream=True)

                # --- æ ¸å¿ƒæ”¹è¿›ï¼šç²¾ç¡®æ§åˆ¶æ¸…é™¤æ—¶æœº ---
                for chunk in stream:
                    content = chunk['message']['content']
                    
                    # åªæœ‰å½“æ¨¡å‹çœŸæ­£è¾“å‡ºäº†å†…å®¹ï¼ˆä¸”éç©ºï¼‰æ—¶ï¼Œæ‰æ¸…é™¤â€œæ€è€ƒä¸­â€æç¤º
                    if content.strip() and not full_response:
                        thinking_container.empty()
                    
                    full_response += content
                    # åŠ¨æ€å±•ç¤ºæ‰“å­—æœºæ•ˆæœ
                    response_placeholder.markdown(full_response + "â–Œ")
                
                # å®Œæˆè¾“å‡ºï¼Œç§»é™¤å…‰æ ‡
                response_placeholder.markdown(full_response)
                
            except Exception as e:
                thinking_container.empty()
                st.error(f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")

        # 3. è¯æ®æº¯æºæ˜¾ç¤º (ä¼˜åŒ–ç‰ˆ)
        if graph_data or vector_docs:
            with st.expander("ğŸ” åŸå§‹æ£€ç´¢è¯æ®"):
                tab1, tab2 = st.tabs(["å›¾è°±äº‹å® (é“¾å¼å™è¿°)", "æ–‡æ¡£ç‰‡æ®µ"])
                
                with tab1:
                    # === [ä¿®æ”¹] ä½¿ç”¨ format_graph_data æ¸²æŸ“ ===
                    if graph_data:
                        # å¤ç”¨ä¹‹å‰è®¡ç®—å¥½çš„ intent
                        current_intent = intent_res.get('intent', 'Info_Query')
                        formatted_text = format_graph_data(graph_data, current_intent)
                        st.markdown(formatted_text)
                    else:
                        st.info("æ— å›¾è°±æ•°æ®")
                
                with tab2:
                    if vector_docs:
                        for i, d in enumerate(vector_docs):
                            st.info(f"**ç‰‡æ®µ {i+1}**:\n{d}")
                    else:
                        st.info("æ— æ–‡æ¡£æ•°æ®")

    # å°†åŠ©æ‰‹çš„å›ç­”å­˜å…¥å¯¹è¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})